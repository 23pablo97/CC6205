{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivación\" data-toc-modified-id=\"Motivación-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Motivación</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Bag of Words</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word Embeddings</a></span></li><li><span><a href=\"#Word2vec\" data-toc-modified-id=\"Word2vec-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word2vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Skip-gram\" data-toc-modified-id=\"Skip-gram-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Skip-gram</a></span></li></ul></li><li><span><a href=\"#Detalles-del-Modelo\" data-toc-modified-id=\"Detalles-del-Modelo-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Detalles del Modelo</a></span></li><li><span><a href=\"#La-capa-Oculta\" data-toc-modified-id=\"La-capa-Oculta-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>La capa Oculta</a></span></li><li><span><a href=\"#Fuentes\" data-toc-modified-id=\"Fuentes-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Fuentes</a></span></li></ul></li><li><span><a href=\"#Entrenar-nuestros-Embeddings\" data-toc-modified-id=\"Entrenar-nuestros-Embeddings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Entrenar nuestros Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset-desde-el-archivo\" data-toc-modified-id=\"Cargar-el-dataset-desde-el-archivo-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Cargar el dataset desde el archivo</a></span></li><li><span><a href=\"#Limpiar-las-noticias\" data-toc-modified-id=\"Limpiar-las-noticias-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Limpiar las noticias</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comenzamos-el-preprocesamiento:\" data-toc-modified-id=\"Comenzamos-el-preprocesamiento:-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Comenzamos el preprocesamiento:</a></span></li></ul></li></ul></li><li><span><a href=\"#Extracción-de-Frases\" data-toc-modified-id=\"Extracción-de-Frases-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extracción de Frases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Separamos-todos-los-documentos-en-tokens.\" data-toc-modified-id=\"Separamos-todos-los-documentos-en-tokens.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Separamos todos los documentos en tokens.</a></span></li><li><span><a href=\"#Buscamos-bigramas-relevantes\" data-toc-modified-id=\"Buscamos-bigramas-relevantes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Buscamos bigramas relevantes</a></span></li><li><span><a href=\"#Retokenizar-el-corpus\" data-toc-modified-id=\"Retokenizar-el-corpus-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Retokenizar el corpus</a></span></li></ul></li><li><span><a href=\"#Entrenamiento-del-Modelo\" data-toc-modified-id=\"Entrenamiento-del-Modelo-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Entrenamiento del Modelo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Construir-el-vocabulario\" data-toc-modified-id=\"Construir-el-vocabulario-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Construir el vocabulario</a></span></li><li><span><a href=\"#Entrenar-el-Modelo\" data-toc-modified-id=\"Entrenar-el-Modelo-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Entrenar el Modelo</a></span></li><li><span><a href=\"#Guardar-y-cargar-el-modelo\" data-toc-modified-id=\"Guardar-y-cargar-el-modelo-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Guardar y cargar el modelo</a></span></li></ul></li><li><span><a href=\"#Tareas:-Palabras-mas-similares-y-Analogías\" data-toc-modified-id=\"Tareas:-Palabras-mas-similares-y-Analogías-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tareas: Palabras mas similares y Analogías</a></span><ul class=\"toc-item\"><li><span><a href=\"#Palabras-mas-similares\" data-toc-modified-id=\"Palabras-mas-similares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Palabras mas similares</a></span></li></ul></li><li><span><a href=\"#Analogías\" data-toc-modified-id=\"Analogías-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analogías</a></span></li><li><span><a href=\"#Visualizar\" data-toc-modified-id=\"Visualizar-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Visualizar</a></span></li><li><span><a href=\"#Visualizamos\" data-toc-modified-id=\"Visualizamos-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Visualizamos</a></span></li><li><span><a href=\"#Word-Embeddings-como-características-para-clasificar\" data-toc-modified-id=\"Word-Embeddings-como-características-para-clasificar-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Word Embeddings como características para clasificar</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dividimos-el-dataset-de-embeddings-en-training-y-test\" data-toc-modified-id=\"Dividimos-el-dataset-de-embeddings-en-training-y-test-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Dividimos el dataset de embeddings en training y test</a></span></li><li><span><a href=\"#Entrenamos-el-clasificador\" data-toc-modified-id=\"Entrenamos-el-clasificador-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Entrenamos el clasificador</a></span></li><li><span><a href=\"#Predecimos-y-evaluamos:\" data-toc-modified-id=\"Predecimos-y-evaluamos:-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Predecimos y evaluamos:</a></span></li></ul></li><li><span><a href=\"#En-comparación-con-BoW\" data-toc-modified-id=\"En-comparación-con-BoW-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>En comparación con BoW</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 2 - Word Embeddings\n",
    "\n",
    "\n",
    "La clase auxiliar de esta semana tendrá varios objetivos: \n",
    "\n",
    "- Entender lo básico de word embeddings \n",
    "- Entrenar nuestros propios `word embeddings` usando el dataset de noticias de la radio biobio. \n",
    "- Experimentar y visualizar los embeddings entrenados. \n",
    "- Por último, resolver la misma tarea de topic classification de la clase auxiliar 1, pero usando los embeddings que hemos calculado. \n",
    "\n",
    "\n",
    "\n",
    "## Motivación\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Pensemos en estas 2 frases como documentos (dieciocheros 🥟🥟🍷):\n",
    "\n",
    "    ¡Estuvo buena esa empanada !\n",
    "    ¡Estuvo espectacular esa empanada!\n",
    "\n",
    "En la practica, sabemos que significan lo mismo.\n",
    "\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "Para esto, generamos un modelo `Bag of Words` sobre cada documento (es decir, transformamos cada palabra a un vector one-hot):\n",
    "\n",
    "$$v = \\{estuvo, buena, esa, empanada, espectacular\\}$$\n",
    "\n",
    "Entonces, el doc 1 quedará:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el doc 2 quedará:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**¿Cuál es el problema?**\n",
    "\n",
    "`buena` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan en la práctica lo mismo, \n",
    "\n",
    "pero en esta representación **son totalmente distintos**, ya que ocupan distintas dimensiones en el modelo. Esto hace que en la práctica, que `buena` y `espectacular` sean tan distintas como `estuvo` y `empanada`. Esto evidentemente, repercute en la calidad de los modelos que creamos.\n",
    "\n",
    "\n",
    "Nos gustaría que eso no sucediera. Que existiera algún método que nos permitiera hacer que palabras similares tengan representaciones similares...\n",
    "\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Pensemos un poco en la `hipótesis distribucional`. Esta plantea que:\n",
    "\n",
    "    \"palabras que ocurren en similares contextos tienden a tener significados similares\" \n",
    "\n",
    "¿Podríamos crear algúna representación vectorial que capture los contextos de las palabras? - La respuesta es si. Estos son los **Word Embeddings**. \n",
    "\n",
    "La idea principal de Word Embeddings (O *Word vectors*) es basarse en la hipotesis distribuciónal para crear, por cada palabra del vocabulario, representaciones vectoriales continuas que capturen su contexto.\n",
    "\n",
    "\n",
    "Es decir, en nuestro ejemplo, `buena` y `espectacular` ocurren en el mismo contexto, por lo que los word embeddings que los deberían representan deberían ser muy similares (ejemplos de mentira hechos a mano*):\n",
    "\n",
    "`buena` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `empanada`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cuál es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¿Cómo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "\n",
    "###  Word2vec\n",
    "\n",
    "Word2Vec es una de las herramientas que nos permitiran construir estos vectores. Consiste en entrenar una **red neuronal de solo 1 capa oculta** a través de la resolución de una task auxiliar: `Skip-Gram`\n",
    "\n",
    "¿Por qué auxiliar?: Porque la usaremos solo para entrenar los pesos de la capa oculta de red. Una vez entrenada, nunca mas haremos Skip-Gram!. \n",
    "\n",
    "#### Skip-gram\n",
    "\n",
    "En pocas palabras, la task a resolver es la siguiente: \n",
    "\n",
    "- Toma una oración cualquiera del corpus. \n",
    "\n",
    "- Dada una palabra cualquiera en la oración, mira las palabras cercanas a esta (dentro de una ventana definida) y toma una aleatoria. \n",
    "\n",
    "- La red deberá predecir que tan probable es que esta palabra sea cercana a la palabra que escogimos.\n",
    "\n",
    "\n",
    "Por ejemplo, para la típica frase: \n",
    "\n",
    "     “The quick brown fox jumps over the lazy dog.”\n",
    "     \n",
    "Usando una ventana de 2 palabras, tendremos: \n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "Esto es lo que luego usará la red para aprender las relaciones entre las palabras.\n",
    "\n",
    "### Detalles del Modelo\n",
    "\n",
    "\n",
    "Pensemos por ejemplo, en un paso del entrenamiento: \n",
    "\n",
    "El vector de entrada será un One-hot de la palabra que estemos viendo en ese momento. En este caso, `ants`.\n",
    "\n",
    "La red, usando su capa oculta, nos entregará la probabilidad de la que la palabra que estamos viendo aparezca con respecto a cada palabra del vocabulario.\n",
    "\n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "Nota: Esto es computacionalmente una locura, ya que por cada palabra debemos calcular la probabilidad de aparición de todas las otras. Imaginen el caso de un vocabulario de 10 millones de palabras...\n",
    "En clases se verá como se puede hacer esto de forma eficiente.\n",
    "\n",
    "Por ende, la salida será un vector que representará una distribución de probabilidades. \n",
    "\n",
    "Durante el entrenamiento, dicho vector será comparado con las estadísticas reales de co-ocurrencia de las palabras.\n",
    "\n",
    "### La capa Oculta\n",
    "\n",
    "Al terminar el entrenamiento, ¿Qué nos queda en la capa oculta?\n",
    "\n",
    "Básicamente, una matriz de $v$ filas por $300$ columnas, la cual contiene lo que buscabamos: Una representación continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representación continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)\n",
    "\n",
    "¿Cómo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot del BoW y las multiplicamos por la matriz:\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "\n",
    "Word2vec:\n",
    "- mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "Gensim: \n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "Nota: Todas las imagenes pertenecen a [Chris McCormick](http://mccormickml.com/about/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementación de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:09.569938Z",
     "start_time": "2019-09-30T14:42:06.019954Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# spacy\n",
    "import spacy \n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Cargar modelos de spacy en español.\n",
    "nlp = spacy.load(\"es_core_news_sm\",  disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset desde el archivo\n",
    "\n",
    "Nota: Pandas descomprime por si mismo el archivo bz2. Pueden descomprimirlo manualmente usando 7zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.910436Z",
     "start_time": "2019-09-30T14:42:09.569938Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\n",
    "    'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_clean.bz2',\n",
    "    encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T15:44:59.327518Z",
     "start_time": "2019-08-26T15:44:59.312300Z"
    }
   },
   "source": [
    "**Examinemos un par de ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.950429Z",
     "start_time": "2019-09-30T14:42:17.910436Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "      <th>publication_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22459</th>\n",
       "      <td>Valentina González</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Juez de Garantía de Temuco presenta demanda in...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-la-araucania</td>\n",
       "      <td>Una demanda indemnizatoria en contra del fis...</td>\n",
       "      <td>[#Demanda, #Juez de Garantía, #Luis Olivares A...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1564774560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22415</th>\n",
       "      <td>César Vega Martínez</td>\n",
       "      <td>/lista/autores/cevega</td>\n",
       "      <td>Rosa Bernile Nienau: así fue la cálida amistad...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/m...</td>\n",
       "      <td>sociedad</td>\n",
       "      <td>misterios</td>\n",
       "      <td>Fue en noviembre pasado cuando la casa de su...</td>\n",
       "      <td>[#Adolf Hitler, #Amistad, #borman, #Fotos, #ho...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1549106040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author               author_link  \\\n",
       "22459   Valentina González  /lista/autores/vgonzalez   \n",
       "22415  César Vega Martínez     /lista/autores/cevega   \n",
       "\n",
       "                                                   title  \\\n",
       "22459  Juez de Garantía de Temuco presenta demanda in...   \n",
       "22415  Rosa Bernile Nienau: así fue la cálida amistad...   \n",
       "\n",
       "                                                    link  category  \\\n",
       "22459  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "22415  https://www.biobiochile.cl/noticias/sociedad/m...  sociedad   \n",
       "\n",
       "                  subcategory  \\\n",
       "22459  region-de-la-araucania   \n",
       "22415               misterios   \n",
       "\n",
       "                                                 content  \\\n",
       "22459    Una demanda indemnizatoria en contra del fis...   \n",
       "22415    Fue en noviembre pasado cuando la casa de su...   \n",
       "\n",
       "                                                    tags  \\\n",
       "22459  [#Demanda, #Juez de Garantía, #Luis Olivares A...   \n",
       "22415  [#Adolf Hitler, #Amistad, #borman, #Fotos, #ho...   \n",
       "\n",
       "                                          embedded_links  publication_datetime  \n",
       "22459  [https://media.biobiochile.cl/wp-content/uploa...         1564774560000  \n",
       "22415                                                 []         1549106040000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar las noticias\n",
    "\n",
    "Primero, vamos a definir la función que definirá como será limpiado y tokenizado cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.970369Z",
     "start_time": "2019-09-30T14:42:17.950429Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Tokenizar y remover  stopwords\n",
    "    txt = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación:  `Word2Vec` usa el contexto de las oraciones para aprender las representaciones de las palabras.\n",
    "Si la oración es muy pequeña, entonces, el entrenamiento no podrá inferir nada. Por eso, se retorna la oración solo cuando hay mas de 2 palabras dentro de la oración.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comenzamos el preprocesamiento:\n",
    "\n",
    "Primero convertimos todas las palabras del documento en minúsculas y realizamos una limpieza rápida de símbolos y espacios extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:42:17.987662Z",
     "start_time": "2019-09-30T14:42:17.970369Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_cleaned_docs = (re.sub(r'[^\\w\\s+]', '', str(row)).lower() for row in data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, tokenizamos el texto usando la función que definimos unas celdas mas arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:33.809990Z",
     "start_time": "2019-09-30T14:42:17.989775Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(pre_cleaned_docs, batch_size=5000, n_threads=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:33.829847Z",
     "start_time": "2019-09-30T14:43:33.809990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de texto procesado:\n",
      "\n",
      "   noticia desarrollo   recopilando antecedentes noticia quédate atento a actualizaciones     estructura restante casa cayó valparaíso colapsó mañana miércoles   produjo a 1015 esquina aldunate huito murieron personas   información preliminar rescatistas labores suspendido peligro seguir trabajando habría lesionados  \n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo de texto procesado:\\n\\n{}\".format(txt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, almacenamos los documentos tokenizados en un `DataFrame` y eliminamos vacios/ducplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:34.120021Z",
     "start_time": "2019-09-30T14:43:33.829847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26265, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "## Extracción de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`.\n",
    "\n",
    "\n",
    "### Separamos todos los documentos en tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:34.755160Z",
     "start_time": "2019-09-30T14:43:34.120021Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:34.774731Z",
     "start_time": "2019-09-30T14:43:34.755160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noticia',\n",
       " 'desarrollo',\n",
       " 'recopilando',\n",
       " 'antecedentes',\n",
       " 'noticia',\n",
       " 'quédate',\n",
       " 'atento',\n",
       " 'a',\n",
       " 'actualizaciones',\n",
       " 'estructura',\n",
       " 'restante',\n",
       " 'casa',\n",
       " 'cayó',\n",
       " 'valparaíso',\n",
       " 'colapsó',\n",
       " 'mañana',\n",
       " 'miércoles',\n",
       " 'produjo',\n",
       " 'a',\n",
       " '1015',\n",
       " 'esquina',\n",
       " 'aldunate',\n",
       " 'huito',\n",
       " 'murieron',\n",
       " 'personas',\n",
       " 'información',\n",
       " 'preliminar',\n",
       " 'rescatistas',\n",
       " 'labores',\n",
       " 'suspendido',\n",
       " 'peligro',\n",
       " 'seguir',\n",
       " 'trabajando',\n",
       " 'habría',\n",
       " 'lesionados']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para ver como quedan las noticias, quitar comentario a la siguiente linea:\n",
    "sent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscamos bigramas relevantes\n",
    "\n",
    "Ahora, buscamos los bigramas relevantes dentro de nuesto corpus usando `Phrases`. La condición para que sean considerados es que aparezcan por lo menos 30 veces repetidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:43:44.724328Z",
     "start_time": "2019-09-30T14:43:34.779249Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:43:34: collecting all words and their counts\n",
      "INFO - 11:43:34: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 11:43:36: PROGRESS: at sentence #5000, processed 1095422 words and 759116 word types\n",
      "INFO - 11:43:38: PROGRESS: at sentence #10000, processed 1923387 words and 1200422 word types\n",
      "INFO - 11:43:40: PROGRESS: at sentence #15000, processed 2814795 words and 1627467 word types\n",
      "INFO - 11:43:42: PROGRESS: at sentence #20000, processed 3801078 words and 2167896 word types\n",
      "INFO - 11:43:44: PROGRESS: at sentence #25000, processed 4786655 words and 2672007 word types\n",
      "INFO - 11:43:44: collected 2802254 word types from a corpus of 5045011 words (unigram + bigrams) and 26265 sentences\n",
      "INFO - 11:43:44: using 2802254 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenizar el corpus\n",
    "\n",
    "Por último, usando `Phraser`, re-tokenizamos el corpus con los bigramas encontrados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:44:12.045998Z",
     "start_time": "2019-09-30T14:43:44.724328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:43:44: source_vocab length 2802254\n",
      "INFO - 11:44:12: Phraser built with 4039 4039 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:44:12.066117Z",
     "start_time": "2019-09-30T14:44:12.048035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noticia',\n",
       " 'desarrollo',\n",
       " 'recopilando',\n",
       " 'antecedentes',\n",
       " 'noticia',\n",
       " 'quédate',\n",
       " 'atento',\n",
       " 'a',\n",
       " 'actualizaciones',\n",
       " 'estructura',\n",
       " 'restante',\n",
       " 'casa',\n",
       " 'cayó',\n",
       " 'valparaíso',\n",
       " 'colapsó',\n",
       " 'mañana_miércoles',\n",
       " 'produjo',\n",
       " 'a',\n",
       " '1015',\n",
       " 'esquina',\n",
       " 'aldunate',\n",
       " 'huito',\n",
       " 'murieron',\n",
       " 'personas',\n",
       " 'información_preliminar',\n",
       " 'rescatistas',\n",
       " 'labores',\n",
       " 'suspendido',\n",
       " 'peligro',\n",
       " 'seguir_trabajando',\n",
       " 'habría',\n",
       " 'lesionados']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para ver como quedan las noticias retokenizadas, quitar comentario a la siguiente linea:\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos parámetros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada\n",
    "- `size` : El tamaño de los embeddings que crearemos. Por lo general, se utilizan 300\n",
    "- `workers`: Cantidad de CPU que serán utilizadas en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.103684Z",
     "start_time": "2019-09-30T14:28:05.143Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v = Word2Vec(min_count=10,\n",
    "                      window=2,\n",
    "                      size=300,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se creará un conjunto que contendrá (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.103684Z",
     "start_time": "2019-09-30T14:28:06.021Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.build_vocab(sentences, progress_per=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuación, entenaremos el modelo. \n",
    "Los parámetros que usaremos serán: \n",
    "\n",
    "- `total_examples`: Número de documentos.\n",
    "- `epochs`: Número de veces que se iterará sobre el corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.103684Z",
     "start_time": "2019-09-30T14:28:08.228Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=30, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. \n",
    "Esto nos permitirá ejecutar eficientemente las tareas que realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:28:17.113824Z",
     "start_time": "2019-09-30T14:28:10.313Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:37:18.951314Z",
     "start_time": "2019-09-30T14:37:16.429053Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Si entrenaste el modelo y lo quieres guardar, descomentar el siguiente bloque.\n",
    "\n",
    "\"\"\"\n",
    "if not os.path.exists('./pretrained_models'):\n",
    "    os.mkdir('./pretrained_models')\n",
    "biobio_w2v.save('./pretrained_models/biobio_w2v.model')\n",
    "\"\"\"\n",
    "\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "#biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:48:36.608370Z",
     "start_time": "2019-09-30T14:48:16.858951Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:48:36: loading Word2VecKeyedVectors object from ./pretrained_models/biobio_w2v.model\n",
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning:\n",
      "\n",
      "This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "\n",
      "INFO - 11:48:36: loading wv recursively from ./pretrained_models/biobio_w2v.model.wv.* with mmap=r\n",
      "INFO - 11:48:36: loading vectors from ./pretrained_models/biobio_w2v.model.wv.vectors.npy with mmap=r\n",
      "INFO - 11:48:36: setting ignored attribute vectors_norm to None\n",
      "INFO - 11:48:36: loading vocabulary recursively from ./pretrained_models/biobio_w2v.model.vocabulary.* with mmap=r\n",
      "INFO - 11:48:36: loading trainables recursively from ./pretrained_models/biobio_w2v.model.trainables.* with mmap=r\n",
      "INFO - 11:48:36: loading syn1neg from ./pretrained_models/biobio_w2v.model.trainables.syn1neg.npy with mmap=r\n",
      "INFO - 11:48:36: setting ignored attribute cum_table to None\n",
      "INFO - 11:48:36: loaded ./pretrained_models/biobio_w2v.model\n"
     ]
    }
   ],
   "source": [
    "# descargar el modelo desde github\n",
    "def read_model_from_github(url):\n",
    "    if not os.path.exists('./pretrained_models'):\n",
    "        os.mkdir('./pretrained_models')\n",
    "\n",
    "    r = requests.get(url)\n",
    "    filename = url.split('/')[-1]\n",
    "    with open('./pretrained_models/' + filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return True\n",
    "\n",
    "\n",
    "[\n",
    "    read_model_from_github(file) for file in [\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model',\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model.trainables.syn1neg.npy',\n",
    "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model.wv.vectors.npy'\n",
    "    ]\n",
    "]\n",
    "# cargar el modelo (si es que lo entrenaron desde local.)\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas: Palabras mas similares y Analogías\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la información contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matemático, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:48:51.768321Z",
     "start_time": "2019-09-30T14:48:51.668750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:48:51: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gato', 0.47238367795944214),\n",
       " ('cachorro', 0.45501261949539185),\n",
       " ('canino', 0.4421420991420746),\n",
       " ('felino', 0.4251091778278351),\n",
       " ('gatito', 0.4125193655490875),\n",
       " ('animal', 0.40201663970947266),\n",
       " ('perrita', 0.39842987060546875),\n",
       " ('zorro', 0.39238905906677246),\n",
       " ('mascota', 0.39086073637008667),\n",
       " ('perros', 0.38970085978507996)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"perro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:48:55.968331Z",
     "start_time": "2019-09-30T14:48:55.948531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald_trump', 0.6679385304450989),\n",
       " ('casa_blanca', 0.6449583768844604),\n",
       " ('unidos', 0.6023310422897339),\n",
       " ('washington', 0.5866063833236694),\n",
       " ('mandatario_estadounidense', 0.5600548386573792),\n",
       " ('presidente_donald', 0.5553752183914185),\n",
       " ('presidente_estadounidense', 0.5523015856742859),\n",
       " ('inquilino', 0.5197651386260986),\n",
       " ('unidos_donald', 0.5129491686820984),\n",
       " ('sessions', 0.5118564367294312)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:00.208512Z",
     "start_time": "2019-09-30T14:49:00.188447Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fidel_castro', 0.382219135761261),\n",
       " ('1959', 0.3599476218223572),\n",
       " ('islámica', 0.35400423407554626),\n",
       " ('marxismo', 0.3403622508049011),\n",
       " ('sandinista', 0.3341415524482727),\n",
       " ('1979', 0.31937718391418457),\n",
       " ('transformación', 0.31889623403549194),\n",
       " ('1917', 0.3090049624443054),\n",
       " ('burguesía', 0.30192387104034424),\n",
       " ('yucatán', 0.29723119735717773)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revolución\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:04.408330Z",
     "start_time": "2019-09-30T14:49:04.388759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pablo_vidal', 0.6696338057518005),\n",
       " ('orsini', 0.639187216758728),\n",
       " ('jorge_brito', 0.6381471157073975),\n",
       " ('frenteamplista', 0.6183565855026245),\n",
       " ('rd', 0.6119238138198853),\n",
       " ('prd', 0.598182201385498),\n",
       " ('demócratacristiano', 0.5970538258552551),\n",
       " ('maite', 0.5770610570907593),\n",
       " ('vlado', 0.5761114954948425),\n",
       " ('partido_democracia', 0.5688214302062988)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revolución_democrática\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:08.528555Z",
     "start_time": "2019-09-30T14:49:08.508579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hut', 0.6154688000679016),\n",
       " ('hamburguesas', 0.4615939259529114),\n",
       " ('corner', 0.4598219394683838),\n",
       " ('sandwich', 0.4314725399017334),\n",
       " ('autoservicio', 0.42904579639434814),\n",
       " ('repartidor', 0.4275974631309509),\n",
       " ('cafeterías', 0.42377257347106934),\n",
       " ('ekono', 0.4233115315437317),\n",
       " ('vodka', 0.4229376018047333),\n",
       " ('comida_rápida', 0.4187615215778351)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:12.623680Z",
     "start_time": "2019-09-30T14:49:12.603628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cabify', 0.5361112952232361),\n",
       " ('eats', 0.49480363726615906),\n",
       " ('beat', 0.4774724245071411),\n",
       " ('transporte_pasajeros', 0.4051392078399658),\n",
       " ('rappi', 0.38707831501960754),\n",
       " ('didi', 0.3694966435432434),\n",
       " ('vtc', 0.3587350845336914),\n",
       " ('aplicaciones', 0.35364559292793274),\n",
       " ('glovo', 0.35351526737213135),\n",
       " ('taxi', 0.3341372013092041)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"uber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:16.865324Z",
     "start_time": "2019-09-30T14:49:16.848729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gigante_chino', 0.5173695087432861),\n",
       " ('zte', 0.5074669122695923),\n",
       " ('directora_financiera', 0.4465712308883667),\n",
       " ('5_g', 0.4312366247177124),\n",
       " ('china', 0.4225515127182007),\n",
       " ('meng_wanzhou', 0.4068417549133301),\n",
       " ('pekin', 0.38649022579193115),\n",
       " ('telecomunicaciones', 0.38293468952178955),\n",
       " ('teléfonos_inteligentes', 0.3672794699668884),\n",
       " ('chips', 0.36506158113479614)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"huawei\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:20.888567Z",
     "start_time": "2019-09-30T14:49:20.868418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canal_13', 0.4343094825744629),\n",
       " ('mega', 0.36687153577804565),\n",
       " ('presidente_directorio', 0.3579253554344177),\n",
       " ('matinal', 0.3533702492713928),\n",
       " ('cuprífera', 0.336997389793396),\n",
       " ('directorio', 0.3351971507072449),\n",
       " ('orrego', 0.3331133723258972),\n",
       " ('fesur', 0.3310084342956543),\n",
       " ('rapu', 0.3075915575027466),\n",
       " ('comisión_investigadora', 0.3052249550819397)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"tvn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:25.168371Z",
     "start_time": "2019-09-30T14:49:25.148319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intelectualmente', 0.5410281419754028),\n",
       " ('comuneros', 0.5132812857627869),\n",
       " ('pehuenches', 0.4597381353378296),\n",
       " ('comunidades_mapuches', 0.44987913966178894),\n",
       " ('mapuche', 0.4347573518753052),\n",
       " ('aucán', 0.4004420042037964),\n",
       " ('pehuenche', 0.37423285841941833),\n",
       " ('vecinales', 0.3617860972881317),\n",
       " ('ancestrales', 0.3590352535247803),\n",
       " ('hortaliceras', 0.35829445719718933)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"mapuches\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:29.528561Z",
     "start_time": "2019-09-30T14:49:29.508488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('camilo_catrillanca', 0.5488350987434387),\n",
       " ('comunero_mapuche', 0.5073081254959106),\n",
       " ('comunero', 0.4860221743583679),\n",
       " ('único_imputado', 0.4092996120452881),\n",
       " ('chadwick', 0.3861224055290222),\n",
       " ('causa_derivada', 0.3794987201690674),\n",
       " ('tokman', 0.36469513177871704),\n",
       " ('sambuceti', 0.364555686712265),\n",
       " ('luchsinger', 0.35905542969703674),\n",
       " ('causándole', 0.3463499844074249)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"catrillanca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:33.843172Z",
     "start_time": "2019-09-30T14:49:33.827576Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('magíster', 0.47122281789779663),\n",
       " ('universidad_santiago', 0.4577624201774597),\n",
       " ('universidad_concepción', 0.43267691135406494),\n",
       " ('odontología', 0.4286888837814331),\n",
       " ('agronomía', 0.4259319305419922),\n",
       " ('universidad_católica', 0.41985636949539185),\n",
       " ('mba', 0.4132372736930847),\n",
       " ('federación_estudiantes', 0.4114116430282593),\n",
       " ('facultad_ciencias', 0.37800729274749756),\n",
       " ('universidad_andrés', 0.377147912979126)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"universidad_chile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:37.744434Z",
     "start_time": "2019-09-30T14:49:37.726374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ultraderecha', 0.5372439622879028),\n",
       " ('extrema_derecha', 0.506591260433197),\n",
       " ('jair_bolsonaro', 0.4804544150829315),\n",
       " ('fascismo', 0.45208853483200073),\n",
       " ('antiinmigración', 0.452001690864563),\n",
       " ('ultraderechista_liga', 0.45156916975975037),\n",
       " ('excapitán_ejército', 0.444217324256897),\n",
       " ('interior_matteo', 0.4428623914718628),\n",
       " ('matteo_salvini', 0.43859899044036865),\n",
       " ('marine_pen', 0.4377690553665161)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"ultraderechista\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:41.812835Z",
     "start_time": "2019-09-30T14:49:41.797262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('augusto_pinochet', 0.4894692301750183),\n",
       " ('19761983', 0.3469034433364868),\n",
       " ('nostálgico', 0.3454264998435974),\n",
       " ('stroessner', 0.3346376419067383),\n",
       " ('franquista', 0.3176833987236023),\n",
       " ('fascista', 0.31446966528892517),\n",
       " ('torturador', 0.31268107891082764),\n",
       " ('19641985', 0.30675196647644043),\n",
       " ('anastasio', 0.3066090941429138),\n",
       " ('exdictador', 0.30496591329574585)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pinochet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:45.668317Z",
     "start_time": "2019-09-30T14:49:45.652696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vilos', 0.46391141414642334),\n",
       " ('coquimbo', 0.4321473240852356),\n",
       " ('santiago', 0.4237024188041687),\n",
       " ('valle_elqui', 0.40091782808303833),\n",
       " ('pica', 0.3972700834274292),\n",
       " ('ovalle', 0.3957822620868683),\n",
       " ('puerto_natales', 0.3800183832645416),\n",
       " ('balmaceda', 0.3758486211299896),\n",
       " ('chillán', 0.37579774856567383),\n",
       " ('gustavo_fricke', 0.3721628785133362)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"serena\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogías\n",
    "\n",
    "Por otra parte, la analogía consiste en comparar 3 terminos mediante una operación del estilo: \n",
    "\n",
    "$$palabra1 - palabra2 \\approx palabra 3 - x$$\n",
    "\n",
    "para encontrar relaciones entre estos.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| palabra 1 (pos) |  palabra 2 (neg) |\n",
    "|-----------------|------------------|\n",
    "|  macri          | piñera           |\n",
    "| chile           |  x               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:50.092063Z",
     "start_time": "2019-09-30T14:49:50.061262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brasil', 0.46593397855758667),\n",
       " ('jair_bolsonaro', 0.4054754078388214),\n",
       " ('excapitán_ejército', 0.35641491413116455)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"bolsonaro\", \"argentina\"], negative=['macri'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:54.192597Z",
     "start_time": "2019-09-30T14:49:54.176976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('walmart_chile', 0.2904326319694519),\n",
       " ('lipigas', 0.28741344809532166),\n",
       " ('empresa', 0.286331444978714)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"chile\", \"huawei\"], negative=['china'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar\n",
    "\n",
    "Para visualizar, usaremos una técnica de reducción de dimensionalidad llamada [`T-SNE`](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "\n",
    "Básicamente, ejecuta una reducción de dimensionalidad, que transforma las 300 dimensiones de los embeddings en 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:49:48.053701Z",
     "start_time": "2019-09-30T14:49:06.721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutamos la transformación. Por lo general, demora una media hora mas o menos...\n",
    "X_embedded = TSNE(n_components=2, verbose=True).fit_transform(\n",
    "    biobio_w2v.wv[biobio_w2v.wv.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la frecuencia de las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:16.690620Z",
     "start_time": "2019-08-29T17:39:16.666685Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = dict()\n",
    "for item in biobio_w2v.wv.vocab:\n",
    "    word_counts[item]=biobio_w2v.wv.vocab[item].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nuevos vectores que nos entrego TSNE mas el vocabulario y la frecuencia del vocabulario, formamos un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:22.784472Z",
     "start_time": "2019-08-29T17:39:22.757545Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = pd.DataFrame({\n",
    "    'x': X_embedded[:,0],\n",
    "    'y': X_embedded[:,1],\n",
    "    'vocab': list(biobio_w2v.wv.vocab.keys()) , \n",
    "    'count': list(word_counts.values())\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guardar y Cargar TSNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:28.809083Z",
     "start_time": "2019-09-30T14:50:26.161920Z"
    }
   },
   "outputs": [],
   "source": [
    "#Guardar\n",
    "#tsne.to_csv('./pretrained_models/biobio_w2v_tsne.csv', index= False, index_label=False)\n",
    "# Cargar\n",
    "#tsne = pd.read_csv('./pretrained_models/biobio_w2v_tsne.csv')\n",
    "\n",
    "\n",
    "# Cargar desde github\n",
    "tsne = pd.read_csv('https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v_tsne.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos el Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:32.850839Z",
     "start_time": "2019-09-30T14:50:32.835018Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tsne = tsne.sort_values(by=['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos\n",
    "\n",
    "Para la visualización, usaremos plotly. Este nos deja tener una barra interactiva que nos mostrará las 100 palabras mas parecidas a la que escribimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:36.745447Z",
     "start_time": "2019-09-30T14:50:36.729664Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne_sample = sorted_tsne.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:40.844304Z",
     "start_time": "2019-09-30T14:50:40.812831Z"
    }
   },
   "outputs": [],
   "source": [
    "described_tsne = tsne_sample.describe()\n",
    "min_y = described_tsne.y['min']\n",
    "max_y = described_tsne.y['max']\n",
    "\n",
    "min_x =  described_tsne.x['min']\n",
    "max_x =  described_tsne.x['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:08:08.767155Z",
     "start_time": "2019-09-24T18:08:08.754179Z"
    }
   },
   "outputs": [],
   "source": [
    "textbox = widgets.Text(description='Palabra:')\n",
    "\n",
    "def validate(word):\n",
    "    if word in tsne_sample.vocab.values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def response(change):\n",
    "\n",
    "    word = textbox.value.lower()\n",
    "    \n",
    "    if (word == ''):\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = tsne_sample.x.values\n",
    "            fig.data[0].y = tsne_sample.y.values\n",
    "            fig.data[0].text = tsne_sample.vocab.values\n",
    "    else :\n",
    "        if validate(word):\n",
    "\n",
    "            most_similar_words = [word]\n",
    "\n",
    "            for word_tuple in biobio_w2v.wv.most_similar(positive=[word],topn = 100 ):\n",
    "                if word_tuple[0] in tsne_sample.vocab.values:\n",
    "                    most_similar_words.append(word_tuple[0])\n",
    "\n",
    "            filtered_words = [word in most_similar_words for word in tsne_sample.vocab]\n",
    "            temp = tsne_sample.loc[filtered_words]\n",
    "\n",
    "            with fig.batch_update():\n",
    "                fig.data[0].x = temp.x.values\n",
    "                fig.data[0].y = temp.y.values\n",
    "                fig.data[0].text = temp.vocab.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:08:12.877944Z",
     "start_time": "2019-09-24T18:08:11.572647Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.FigureWidget(data=[\n",
    "    go.Scatter(x=tsne_sample.x,\n",
    "               y=tsne_sample.y,\n",
    "               mode='markers',\n",
    "               text=tsne_sample.vocab)\n",
    "],\n",
    "                      layout=go.Layout(title=go.layout.Title(\n",
    "                          text=\"Visualización 2D Embeddings Biobio\"),\n",
    "                                       yaxis =dict(range=[min_y, max_y]),\n",
    "                                       xaxis =dict(range=[min_x, max_x])\n",
    "                                      ))\n",
    "\n",
    "container = widgets.HBox([textbox])\n",
    "textbox.observe(response, names=\"value\")\n",
    "\n",
    "widgets.VBox([container, fig])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T19:55:09.459852Z",
     "start_time": "2019-08-28T19:55:09.454865Z"
    }
   },
   "source": [
    "## Word Embeddings como características para clasificar\n",
    "\n",
    "\n",
    "En esta sección, veremos como utilizar los word embeddings como característica para **clasificar nuevamente el tópico de las noticias de la radio biobio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:58:12.927399Z",
     "start_time": "2019-09-24T12:58:12.921416Z"
    }
   },
   "source": [
    "Definimos el tokenizador (el mismo de antes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:44.986523Z",
     "start_time": "2019-09-30T14:50:44.970867Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    preprocessed_doc = re.sub(r'[^\\w\\s+]', '', str(doc)).lower() \n",
    "    return [x.orth_ for x in nlp(preprocessed_doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos como vamos a representar el documento. En este caso, haremos lo mismo que `BoW`.\n",
    "\n",
    "Es decir, obtendremos la representación vectorial de cada palabra (ahora como vector continuo) y luego las sumaremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:50:49.166630Z",
     "start_time": "2019-09-30T14:50:49.151006Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_wv(wv, document, aggregation_function= np.sum):\n",
    "    tokenized_document = tokenizer(document)\n",
    "    selected_wv = []\n",
    "\n",
    "    for token in tokenized_document:\n",
    "        if token in wv.vocab:\n",
    "            selected_wv.append(wv[token])\n",
    "\n",
    "    selected_wv = np.array(selected_wv)\n",
    "    return aggregation_function(selected_wv, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:52:34.760231Z",
     "start_time": "2019-09-30T14:50:53.362114Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# transformamos cada documento del dataset en la suma de los embeddings.\n",
    "procesed_wv = np.array([get_doc_wv(biobio_w2v.wv, item) for item in data.content.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:25.817110Z",
     "start_time": "2019-09-23T20:43:25.797172Z"
    }
   },
   "source": [
    "### Dividimos el dataset de embeddings en training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:52:38.855864Z",
     "start_time": "2019-09-30T14:52:38.818075Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(procesed_wv,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:24.788311Z",
     "start_time": "2019-09-30T14:52:42.829264Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0,\n",
    "                         solver='saga',\n",
    "                         multi_class='auto',\n",
    "                         max_iter=5000,\n",
    "                         n_jobs=4).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:47.337340Z",
     "start_time": "2019-09-23T20:43:47.317007Z"
    }
   },
   "source": [
    "### Predecimos y evaluamos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:29.446723Z",
     "start_time": "2019-09-30T14:59:29.415481Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:33.639984Z",
     "start_time": "2019-09-30T14:59:33.577449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 789   19  120   10   16]\n",
      " [  24 3205   38    2  120]\n",
      " [  87   29 3130   15   17]\n",
      " [  16    2   65  147    4]\n",
      " [  15  150   24    9  664]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:38.004822Z",
     "start_time": "2019-09-30T14:59:37.750625Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     economia       0.85      0.83      0.84       954\n",
      "internacional       0.94      0.95      0.94      3389\n",
      "     nacional       0.93      0.95      0.94      3278\n",
      "      opinion       0.80      0.63      0.71       234\n",
      "     sociedad       0.81      0.77      0.79       862\n",
      "\n",
      "     accuracy                           0.91      8717\n",
      "    macro avg       0.87      0.83      0.84      8717\n",
      " weighted avg       0.91      0.91      0.91      8717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:57:03.817026Z",
     "start_time": "2019-09-24T12:57:03.814008Z"
    }
   },
   "source": [
    "## En comparación con BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:41.956520Z",
     "start_time": "2019-09-30T14:59:41.934383Z"
    }
   },
   "outputs": [],
   "source": [
    "def classificate_with_bow(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Tokenizar y lematizar.\n",
    "    def tokenizer_with_lemmatization(doc):\n",
    "        return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "    # Definimos el vectorizador para convertir el texto a BoW:\n",
    "    vectorizer = CountVectorizer(analyzer='word',\n",
    "                                 tokenizer=tokenizer_with_lemmatization,\n",
    "                                 \n",
    "                                 ngram_range=(1, 1))\n",
    "\n",
    "    # Definimos el clasificador que usaremos.\n",
    "    clf = LogisticRegression(solver='saga',\n",
    "                             multi_class='ovr',\n",
    "                             max_iter=5000,\n",
    "                             n_jobs=4)\n",
    "\n",
    "    # Definimos el pipeline\n",
    "    text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])\n",
    "\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    clf_report = classification_report(y_test, predicted)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, predicted)\n",
    "    print(clf_report, '\\n\\n', conf_matrix)\n",
    "\n",
    "    return predicted, conf_matrix, clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:59:45.765054Z",
     "start_time": "2019-09-30T14:59:45.749434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data.content,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:09:04.394161Z",
     "start_time": "2019-09-30T14:59:49.646878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     economia       0.88      0.83      0.85       954\n",
      "internacional       0.95      0.96      0.95      3389\n",
      "     nacional       0.94      0.96      0.95      3278\n",
      "      opinion       0.90      0.80      0.85       234\n",
      "     sociedad       0.83      0.82      0.83       862\n",
      "\n",
      "     accuracy                           0.93      8717\n",
      "    macro avg       0.90      0.87      0.89      8717\n",
      " weighted avg       0.93      0.93      0.93      8717\n",
      " \n",
      "\n",
      " [[ 795   13  122    4   20]\n",
      " [  12 3242   33    0  102]\n",
      " [  80   32 3139   12   15]\n",
      " [   8    3   29  188    6]\n",
      " [  13  113   23    4  709]]\n"
     ]
    }
   ],
   "source": [
    "bow_pred, bow_conf_matrix, bow_clf_report = classificate_with_bow(X_train_2, y_train_2, X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:09:08.823694Z",
     "start_time": "2019-09-30T15:09:08.804053Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     economia       0.88      0.83      0.85       954\n",
      "internacional       0.95      0.96      0.95      3389\n",
      "     nacional       0.94      0.96      0.95      3278\n",
      "      opinion       0.90      0.80      0.85       234\n",
      "     sociedad       0.83      0.82      0.83       862\n",
      "\n",
      "     accuracy                           0.93      8717\n",
      "    macro avg       0.90      0.87      0.89      8717\n",
      " weighted avg       0.93      0.93      0.93      8717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bow_clf_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
