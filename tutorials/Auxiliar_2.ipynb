{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Motivación\" data-toc-modified-id=\"Motivación-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Motivación</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Bag of Words</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word Embeddings</a></span></li><li><span><a href=\"#Word2vec\" data-toc-modified-id=\"Word2vec-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word2vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Skip-gram\" data-toc-modified-id=\"Skip-gram-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Skip-gram</a></span></li></ul></li><li><span><a href=\"#Detalles-del-Modelo\" data-toc-modified-id=\"Detalles-del-Modelo-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Detalles del Modelo</a></span></li><li><span><a href=\"#La-capa-Oculta\" data-toc-modified-id=\"La-capa-Oculta-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>La capa Oculta</a></span></li><li><span><a href=\"#Fuentes\" data-toc-modified-id=\"Fuentes-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Fuentes</a></span></li></ul></li><li><span><a href=\"#Entrenar-nuestros-Embeddings\" data-toc-modified-id=\"Entrenar-nuestros-Embeddings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Entrenar nuestros Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cargar-el-dataset-desde-el-archivo\" data-toc-modified-id=\"Cargar-el-dataset-desde-el-archivo-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Cargar el dataset desde el archivo</a></span></li><li><span><a href=\"#Limpiar-las-noticias\" data-toc-modified-id=\"Limpiar-las-noticias-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Limpiar las noticias</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comenzamos-el-preprocesamiento:\" data-toc-modified-id=\"Comenzamos-el-preprocesamiento:-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Comenzamos el preprocesamiento:</a></span></li></ul></li></ul></li><li><span><a href=\"#Extracción-de-Frases\" data-toc-modified-id=\"Extracción-de-Frases-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extracción de Frases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Separamos-todos-los-documentos-en-tokens.\" data-toc-modified-id=\"Separamos-todos-los-documentos-en-tokens.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Separamos todos los documentos en tokens.</a></span></li><li><span><a href=\"#Buscamos-bigramas-relevantes\" data-toc-modified-id=\"Buscamos-bigramas-relevantes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Buscamos bigramas relevantes</a></span></li><li><span><a href=\"#Retokenizar-el-corpus\" data-toc-modified-id=\"Retokenizar-el-corpus-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Retokenizar el corpus</a></span></li></ul></li><li><span><a href=\"#Entrenamiento-del-Modelo\" data-toc-modified-id=\"Entrenamiento-del-Modelo-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Entrenamiento del Modelo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Construir-el-vocabulario\" data-toc-modified-id=\"Construir-el-vocabulario-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Construir el vocabulario</a></span></li><li><span><a href=\"#Entrenar-el-Modelo\" data-toc-modified-id=\"Entrenar-el-Modelo-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Entrenar el Modelo</a></span></li><li><span><a href=\"#Guardar-y-cargar-el-modelo\" data-toc-modified-id=\"Guardar-y-cargar-el-modelo-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Guardar y cargar el modelo</a></span></li></ul></li><li><span><a href=\"#Tareas:-Palabras-mas-similares-y-Analogías\" data-toc-modified-id=\"Tareas:-Palabras-mas-similares-y-Analogías-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tareas: Palabras mas similares y Analogías</a></span><ul class=\"toc-item\"><li><span><a href=\"#Palabras-mas-similares\" data-toc-modified-id=\"Palabras-mas-similares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Palabras mas similares</a></span></li></ul></li><li><span><a href=\"#Analogías\" data-toc-modified-id=\"Analogías-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analogías</a></span></li><li><span><a href=\"#Visualizar\" data-toc-modified-id=\"Visualizar-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Visualizar</a></span></li><li><span><a href=\"#Visualizamos\" data-toc-modified-id=\"Visualizamos-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Visualizamos</a></span></li><li><span><a href=\"#Word-Embeddings-como-características-para-clasificar\" data-toc-modified-id=\"Word-Embeddings-como-características-para-clasificar-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Word Embeddings como características para clasificar</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dividimos-el-dataset-de-embeddings-en-training-y-test\" data-toc-modified-id=\"Dividimos-el-dataset-de-embeddings-en-training-y-test-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Dividimos el dataset de embeddings en training y test</a></span></li><li><span><a href=\"#Entrenamos-el-clasificador\" data-toc-modified-id=\"Entrenamos-el-clasificador-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Entrenamos el clasificador</a></span></li><li><span><a href=\"#Predecimos-y-evaluamos:\" data-toc-modified-id=\"Predecimos-y-evaluamos:-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Predecimos y evaluamos:</a></span></li></ul></li><li><span><a href=\"#En-comparación-con-BoW\" data-toc-modified-id=\"En-comparación-con-BoW-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>En comparación con BoW</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 2 - Word Embeddings\n",
    "\n",
    "\n",
    "La clase auxiliar de esta semana tendrá varios objetivos: \n",
    "\n",
    "- Entender lo básico de word embeddings \n",
    "- Entrenar nuestros propios `word embeddings` usando el dataset de noticias de la radio biobio. \n",
    "- Experimentar y visualizar los embeddings entrenados. \n",
    "- Por último, resolver la misma tarea de topic classification de la clase auxiliar 1, pero usando los embeddings que hemos calculado. \n",
    "\n",
    "\n",
    "\n",
    "## Motivación\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Pensemos en estas 2 frases como documentos (dieciocheros 🥟🥟🍷):\n",
    "\n",
    "    ¡Estuvo buena esa empanada !\n",
    "    ¡Estuvo espectacular esa empanada!\n",
    "\n",
    "En la practica, sabemos que significan lo mismo.\n",
    "\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "Para esto, generamos un modelo `Bag of Words` sobre cada documento (es decir, transformamos cada palabra a un vector one-hot):\n",
    "\n",
    "$$v = \\{estuvo, buena, esa, empanada, espectacular\\}$$\n",
    "\n",
    "Entonces, el doc 1 quedará:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el doc 2 quedará:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**¿Cuál es el problema?**\n",
    "\n",
    "`buena` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan en la práctica lo mismo, \n",
    "\n",
    "pero en esta representación **son totalmente distintos**, ya que ocupan distintas dimensiones en el modelo. Esto hace que en la práctica, que `buena` y `espectacular` sean tan distintas como `estuvo` y `empanada`. Esto evidentemente, repercute en la calidad de los modelos que creamos.\n",
    "\n",
    "\n",
    "Nos gustaría que eso no sucediera. Que existiera algún método que nos permitiera hacer que palabras similares tengan representaciones similares...\n",
    "\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "Pensemos un poco en la `hipótesis distribucional`. Esta plantea que:\n",
    "\n",
    "    \"palabras que ocurren en similares contextos tienden a tener significados similares\" \n",
    "\n",
    "¿Podríamos crear algúna representación vectorial que capture los contextos de las palabras? - La respuesta es si. Estos son los **Word Embeddings**. \n",
    "\n",
    "La idea principal de Word Embeddings (O *Word vectors*) es basarse en la hipotesis distribuciónal para crear, por cada palabra del vocabulario, representaciones vectoriales continuas que capturen su contexto.\n",
    "\n",
    "\n",
    "Es decir, en nuestro ejemplo, `buena` y `espectacular` ocurren en el mismo contexto, por lo que los word embeddings que los deberían representan deberían ser muy similares (ejemplos de mentira hechos a mano*):\n",
    "\n",
    "`buena` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `empanada`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cuál es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¿Cómo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "\n",
    "###  Word2vec\n",
    "\n",
    "Word2Vec es una de las herramientas que nos permitiran construir estos vectores. Consiste en entrenar una **red neuronal de solo 1 capa oculta** a través de la resolución de una task auxiliar: `Skip-Gram`\n",
    "\n",
    "¿Por qué auxiliar?: Porque la usaremos solo para entrenar los pesos de la capa oculta de red. Una vez entrenada, nunca mas haremos Skip-Gram!. \n",
    "\n",
    "#### Skip-gram\n",
    "\n",
    "En pocas palabras, la task a resolver es la siguiente: \n",
    "\n",
    "- Toma una oración cualquiera del corpus. \n",
    "\n",
    "- Dada una palabra cualquiera en la oración, mira las palabras cercanas a esta (dentro de una ventana definida) y toma una aleatoria. \n",
    "\n",
    "- La red deberá predecir que tan probable es que esta palabra sea cercana a la palabra que escogimos.\n",
    "\n",
    "\n",
    "Por ejemplo, para la típica frase: \n",
    "\n",
    "     “The quick brown fox jumps over the lazy dog.”\n",
    "     \n",
    "Usando una ventana de 2 palabras, tendremos: \n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "Esto es lo que luego usará la red para aprender las relaciones entre las palabras.\n",
    "\n",
    "### Detalles del Modelo\n",
    "\n",
    "\n",
    "Pensemos por ejemplo, en un paso del entrenamiento: \n",
    "\n",
    "El vector de entrada será un One-hot de la palabra que estemos viendo en ese momento. En este caso, `ants`.\n",
    "\n",
    "La red, usando su capa oculta, nos entregará la probabilidad de la que la palabra que estamos viendo aparezca con respecto a cada palabra del vocabulario.\n",
    "\n",
    "\n",
    "![Skip Gram](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "Nota: Esto es computacionalmente una locura, ya que por cada palabra debemos calcular la probabilidad de aparición de todas las otras. Imaginen el caso de un vocabulario de 10 millones de palabras...\n",
    "En clases se verá como se puede hacer esto de forma eficiente.\n",
    "\n",
    "Por ende, la salida será un vector que representará una distribución de probabilidades. \n",
    "\n",
    "Durante el entrenamiento, dicho vector será comparado con las estadísticas reales de co-ocurrencia de las palabras.\n",
    "\n",
    "### La capa Oculta\n",
    "\n",
    "Al terminar el entrenamiento, ¿Qué nos queda en la capa oculta?\n",
    "\n",
    "Básicamente, una matriz de $v$ filas por $300$ columnas, la cual contiene lo que buscabamos: Una representación continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representación continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)\n",
    "\n",
    "¿Cómo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot del BoW y las multiplicamos por la matriz:\n",
    "\n",
    "![Capa Oculta](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)\n",
    "\n",
    "\n",
    "### Fuentes\n",
    "\n",
    "Word2vec:\n",
    "- mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "Gensim: \n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "Nota: Todas las imagenes pertenecen a [Chris McCormick](http://mccormickml.com/about/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementación de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:30:46.315573Z",
     "start_time": "2019-09-24T14:30:42.817847Z"
    }
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "\n",
    "# spacy\n",
    "import spacy \n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Cargar modelos de spacy en español.\n",
    "nlp = spacy.load(\"es_core_news_sm\",  disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset desde el archivo\n",
    "\n",
    "Nota: Pandas descomprime por si mismo el archivo bz2. Pueden descomprimirlo manualmente usando 7zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:27.188964Z",
     "start_time": "2019-09-24T14:41:26.549218Z"
    }
   },
   "outputs": [],
   "source": [
    "# leer desde directorio local\n",
    "#data = pd.read_json('./datasets/biobio_clean.bz2', encoding = \"utf-8\")\n",
    "\n",
    "#leer desde github\n",
    "data = pd.read_json('https://github.com/dccuchile/CC6205/blob/master/tutorials/datasets/biobio_clean.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T15:44:59.327518Z",
     "start_time": "2019-08-26T15:44:59.312300Z"
    }
   },
   "source": [
    "**Examinemos un par de ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:32.588592Z",
     "start_time": "2019-09-24T14:41:32.570679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar las noticias\n",
    "\n",
    "Primero, vamos a definir la función que definirá como será limpiado y tokenizado cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:34.588581Z",
     "start_time": "2019-09-24T14:41:34.583595Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Tokenizar y remover  stopwords\n",
    "    txt = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación:  `Word2Vec` usa el contexto de las oraciones para aprender las representaciones de las palabras.\n",
    "Si la oración es muy pequeña, entonces, el entrenamiento no podrá inferir nada. Por eso, se retorna la oración solo cuando hay mas de 2 palabras dentro de la oración.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comenzamos el preprocesamiento:\n",
    "\n",
    "Primero convertimos todas las palabras del documento en minúsculas y realizamos una limpieza rápida de símbolos y espacios extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:41:35.072032Z",
     "start_time": "2019-09-24T14:41:35.067071Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_cleaned_docs = (re.sub(r'[^\\w\\s+]', '', str(row)).lower() for row in data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, tokenizamos el texto usando la función que definimos unas celdas mas arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:42:56.915741Z",
     "start_time": "2019-09-24T14:41:36.315711Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = [cleaning(doc) for doc in nlp.pipe(pre_cleaned_docs, batch_size=5000, n_threads=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:42:57.970692Z",
     "start_time": "2019-09-24T14:42:57.964708Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ejemplo de texto procesado:\\n\\n{}\".format(txt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, almacenamos los documentos tokenizados en un `DataFrame` y eliminamos vacios/ducplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:42:59.216401Z",
     "start_time": "2019-09-24T14:42:58.936111Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "## Extracción de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`.\n",
    "\n",
    "\n",
    "### Separamos todos los documentos en tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:00.805427Z",
     "start_time": "2019-09-24T14:43:00.189761Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:03.630837Z",
     "start_time": "2019-09-24T14:43:03.624853Z"
    }
   },
   "outputs": [],
   "source": [
    "# para ver como quedan las noticias, quitar comentario a la siguiente linea:\n",
    "sent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscamos bigramas relevantes\n",
    "\n",
    "Ahora, buscamos los bigramas relevantes dentro de nuesto corpus usando `Phrases`. La condición para que sean considerados es que aparezcan por lo menos 30 veces repetidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:16.081560Z",
     "start_time": "2019-09-24T14:43:06.488201Z"
    }
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenizar el corpus\n",
    "\n",
    "Por último, usando `Phraser`, re-tokenizamos el corpus con los bigramas encontrados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:46.938115Z",
     "start_time": "2019-09-24T14:43:18.940918Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:49.749575Z",
     "start_time": "2019-09-24T14:43:49.743591Z"
    }
   },
   "outputs": [],
   "source": [
    "# para ver como quedan las noticias retokenizadas, quitar comentario a la siguiente linea:\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos parámetros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada\n",
    "- `size` : El tamaño de los embeddings que crearemos. Por lo general, se utilizan 300\n",
    "- `workers`: Cantidad de CPU que serán utilizadas en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:43:52.676789Z",
     "start_time": "2019-09-24T14:43:52.670767Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v = Word2Vec(min_count=10,\n",
    "                      window=2,\n",
    "                      size=300,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.0007,\n",
    "                      negative=20,\n",
    "                      workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se creará un conjunto que contendrá (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:24:34.585495Z",
     "start_time": "2019-09-24T14:24:19.317916Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.build_vocab(sentences, progress_per=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuación, entenaremos el modelo. \n",
    "Los parámetros que usaremos serán: \n",
    "\n",
    "- `total_examples`: Número de documentos.\n",
    "- `epochs`: Número de veces que se iterará sobre el corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T16:57:49.936306Z",
     "start_time": "2019-08-29T16:48:47.066955Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=30, report_delay=10)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. \n",
    "Esto nos permitirá ejecutar eficientemente las tareas que realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T16:57:56.282837Z",
     "start_time": "2019-08-29T16:57:55.929734Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:49:23.897409Z",
     "start_time": "2019-09-24T14:49:23.462536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Si entrenaste el modelo y lo quieres guardar, eliminar el comentario de la siguiente linea.\n",
    "#biobio_w2v.save('./pretrained_models/biobio_w2v.model')\n",
    "\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n",
    "\n",
    "def load_model_from_github():\n",
    "    import requests\n",
    "    fnames = ['biobio_w2v.model', 'biobio_w2v.model.trainables.syn1neg.npy', 'biobio_w2v.model.wv.vectors.npy']\n",
    "    url = 'https://github.com/dccuchile/CC6205/tree/master/tutorials/pretrained_models/'\n",
    "    for fname in fnames:\n",
    "        r = requests.get(url + fname)\n",
    "        open(fname , 'wb').write(r.content)\n",
    "\n",
    "# cargar desde github (eliminar el comentario)\n",
    "#load_model_from_github()        \n",
    "\n",
    "# cargar desde local o github\n",
    "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas: Palabras mas similares y Analogías\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la información contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matemático, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:49:27.192117Z",
     "start_time": "2019-09-24T14:49:27.104351Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"perro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:42.947043Z",
     "start_time": "2019-09-23T20:13:42.916806Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"trump\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:45.746655Z",
     "start_time": "2019-09-23T20:13:45.726960Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revolución\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:48.487053Z",
     "start_time": "2019-09-23T20:13:48.466879Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"revolución_democrática\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:51.246820Z",
     "start_time": "2019-09-23T20:13:51.227020Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pizza\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:54.006593Z",
     "start_time": "2019-09-23T20:13:53.987031Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"uber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:56.726861Z",
     "start_time": "2019-09-23T20:13:56.706907Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"huawei\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:13:59.521901Z",
     "start_time": "2019-09-23T20:13:59.506636Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"tvn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:51:23.372000Z",
     "start_time": "2019-08-26T21:51:23.363072Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"mapuches\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:51:51.446650Z",
     "start_time": "2019-08-26T21:51:51.438671Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"catrillanca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:52:35.655340Z",
     "start_time": "2019-08-26T21:52:35.645367Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"universidad_chile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:14:06.366867Z",
     "start_time": "2019-09-23T20:14:06.340254Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"ultraderechista\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:14:26.707154Z",
     "start_time": "2019-09-23T20:14:26.687181Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"pinochet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:14:15.636723Z",
     "start_time": "2019-09-23T20:14:15.616965Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"serena\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogías\n",
    "\n",
    "Por otra parte, la analogía consiste en comparar 3 terminos mediante una operación del estilo: \n",
    "\n",
    "$$palabra1 - palabra2 \\approx palabra 3 - x$$\n",
    "\n",
    "para encontrar relaciones entre estos.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| palabra 1 (pos) |  palabra 2 (neg) |\n",
    "|-----------------|------------------|\n",
    "|  macri          | piñera           |\n",
    "| chile           |  x               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:15:14.846789Z",
     "start_time": "2019-09-23T20:15:14.827133Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"bolsonaro\", \"argentina\"], negative=['macri'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:15:27.566927Z",
     "start_time": "2019-09-23T20:15:27.547155Z"
    }
   },
   "outputs": [],
   "source": [
    "biobio_w2v.wv.most_similar(positive=[\"chile\", \"huawei\"], negative=['china'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar\n",
    "\n",
    "Para visualizar, usaremos una técnica de reducción de dimensionalidad llamada [`T-SNE`](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "\n",
    "Básicamente, ejecuta una reducción de dimensionalidad, que transforma las 300 dimensiones de los embeddings en 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:10.628609Z",
     "start_time": "2019-08-29T17:10:44.814442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutamos la transformación. Por lo general, demora una media hora mas o menos...\n",
    "X_embedded = TSNE(n_components=2, verbose=True).fit_transform(\n",
    "    biobio_w2v.wv[biobio_w2v.wv.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la frecuencia de las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:16.690620Z",
     "start_time": "2019-08-29T17:39:16.666685Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = dict()\n",
    "for item in biobio_w2v.wv.vocab:\n",
    "    word_counts[item]=biobio_w2v.wv.vocab[item].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nuevos vectores que nos entrego TSNE mas el vocabulario y la frecuencia del vocabulario, formamos un DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T17:39:22.784472Z",
     "start_time": "2019-08-29T17:39:22.757545Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = pd.DataFrame({\n",
    "    'x': X_embedded[:,0],\n",
    "    'y': X_embedded[:,1],\n",
    "    'vocab': list(biobio_w2v.wv.vocab.keys()) , \n",
    "    'count': list(word_counts.values())\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guardar y Cargar TSNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:14.072636Z",
     "start_time": "2019-09-24T14:50:13.465573Z"
    }
   },
   "outputs": [],
   "source": [
    "#Guardar\n",
    "#tsne.to_csv('./pretrained_models/biobio_w2v_tsne.csv', index= False, index_label=False)\n",
    "# Cargar\n",
    "#tsne = pd.read_csv('./pretrained_models/biobio_w2v_tsne.csv')\n",
    "\n",
    "# Cargar desde github\n",
    "tsne = pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/pretrained_models/biobio_w2v_tsne.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos el Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:16.969679Z",
     "start_time": "2019-09-24T14:50:16.958719Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tsne = tsne.sort_values(by=['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos\n",
    "\n",
    "Para la visualización, usaremos plotly. Este nos deja tener una barra interactiva que nos mostrará las 100 palabras mas parecidas a la que escribimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:19.907893Z",
     "start_time": "2019-09-24T14:50:19.903903Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne_sample = sorted_tsne.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:22.633640Z",
     "start_time": "2019-09-24T14:50:22.613691Z"
    }
   },
   "outputs": [],
   "source": [
    "described_tsne = tsne_sample.describe()\n",
    "min_y = described_tsne.y['min']\n",
    "max_y = described_tsne.y['max']\n",
    "\n",
    "min_x =  described_tsne.x['min']\n",
    "max_x =  described_tsne.x['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:25.381294Z",
     "start_time": "2019-09-24T14:50:25.366339Z"
    }
   },
   "outputs": [],
   "source": [
    "textbox = widgets.Text(description='Palabra:')\n",
    "\n",
    "def validate(word):\n",
    "    if word in tsne_sample.vocab.values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def response(change):\n",
    "\n",
    "    word = textbox.value.lower()\n",
    "    \n",
    "    if (word == ''):\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = tsne_sample.x.values\n",
    "            fig.data[0].y = tsne_sample.y.values\n",
    "            fig.data[0].text = tsne_sample.vocab.values\n",
    "    else :\n",
    "        if validate(word):\n",
    "\n",
    "            most_similar_words = [word]\n",
    "\n",
    "            for word_tuple in biobio_w2v.wv.most_similar(positive=[word],topn = 100 ):\n",
    "                if word_tuple[0] in tsne_sample.vocab.values:\n",
    "                    most_similar_words.append(word_tuple[0])\n",
    "\n",
    "            filtered_words = [word in most_similar_words for word in tsne_sample.vocab]\n",
    "            temp = tsne_sample.loc[filtered_words]\n",
    "\n",
    "            with fig.batch_update():\n",
    "                fig.data[0].x = temp.x.values\n",
    "                fig.data[0].y = temp.y.values\n",
    "                fig.data[0].text = temp.vocab.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:50:29.606877Z",
     "start_time": "2019-09-24T14:50:28.112993Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.FigureWidget(data=[\n",
    "    go.Scatter(x=tsne_sample.x,\n",
    "               y=tsne_sample.y,\n",
    "               mode='markers',\n",
    "               text=tsne_sample.vocab)\n",
    "],\n",
    "                      layout=go.Layout(title=go.layout.Title(\n",
    "                          text=\"Visualización 2D Embeddings Biobio\"),\n",
    "                                       yaxis =dict(range=[min_y, max_y]),\n",
    "                                       xaxis =dict(range=[min_x, max_x])\n",
    "                                      ))\n",
    "\n",
    "container = widgets.HBox([textbox])\n",
    "textbox.observe(response, names=\"value\")\n",
    "\n",
    "widgets.VBox([container, fig])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T19:55:09.459852Z",
     "start_time": "2019-08-28T19:55:09.454865Z"
    }
   },
   "source": [
    "## Word Embeddings como características para clasificar\n",
    "\n",
    "\n",
    "En esta sección, veremos como utilizar los word embeddings como característica para **clasificar nuevamente el tópico de las noticias de la radio biobio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:58:12.927399Z",
     "start_time": "2019-09-24T12:58:12.921416Z"
    }
   },
   "source": [
    "Definimos el tokenizador (el mismo de antes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:31:11.371736Z",
     "start_time": "2019-09-24T13:31:11.366776Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    preprocessed_doc = re.sub(r'[^\\w\\s+]', '', str(doc)).lower() \n",
    "    return [x.orth_ for x in nlp(preprocessed_doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos como vamos a representar el documento. En este caso, haremos lo mismo que `BoW`.\n",
    "\n",
    "Es decir, obtendremos la representación vectorial de cada palabra (ahora como vector continuo) y luego las sumaremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:42:32.875456Z",
     "start_time": "2019-09-24T13:42:32.870442Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_wv(wv, document, aggregation_function= np.mean):\n",
    "    tokenized_document = tokenizer(document)\n",
    "    selected_wv = []\n",
    "\n",
    "    for token in tokenized_document:\n",
    "        if token in wv.vocab:\n",
    "            selected_wv.append(wv[token])\n",
    "\n",
    "    selected_wv = np.array(selected_wv)\n",
    "    return aggregation_function(selected_wv, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:44:03.869709Z",
     "start_time": "2019-09-24T13:42:33.594713Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# transformamos cada documento del dataset en la suma de los embeddings.\n",
    "procesed_wv = np.array([get_doc_wv(biobio_w2v.wv, item) for item in data.content.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:25.817110Z",
     "start_time": "2019-09-23T20:43:25.797172Z"
    }
   },
   "source": [
    "### Dividimos el dataset de embeddings en training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:09.609721Z",
     "start_time": "2019-09-24T13:46:09.581768Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(procesed_wv,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:14.109965Z",
     "start_time": "2019-09-24T13:46:10.839565Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0,\n",
    "                         solver='saga',\n",
    "                         multi_class='auto',\n",
    "                         max_iter=5000,\n",
    "                         n_jobs=4).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:43:47.337340Z",
     "start_time": "2019-09-23T20:43:47.317007Z"
    }
   },
   "source": [
    "### Predecimos y evaluamos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:16.403640Z",
     "start_time": "2019-09-24T13:46:16.395688Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T13:46:17.245827Z",
     "start_time": "2019-09-24T13:46:16.975547Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:57:03.817026Z",
     "start_time": "2019-09-24T12:57:03.814008Z"
    }
   },
   "source": [
    "## En comparación con BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:10:50.593431Z",
     "start_time": "2019-09-24T14:10:50.586451Z"
    }
   },
   "outputs": [],
   "source": [
    "def classificate_with_bow(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Tokenizar y lematizar.\n",
    "    def tokenizer_with_lemmatization(doc):\n",
    "        return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "    # Definimos el vectorizador para convertir el texto a BoW:\n",
    "    vectorizer = CountVectorizer(analyzer='word', tokenizer = tokenizer_with_lemmatization, ngram_range=(1,1))  \n",
    "\n",
    "    # Definimos el clasificador que usaremos.\n",
    "    clf = LogisticRegression(solver='saga', multi_class='ovr', max_iter = 1000, n_jobs=4)   \n",
    "\n",
    "    # Definimos el pipeline\n",
    "    text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])\n",
    "    \n",
    "    text_clf.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = text_clf.predict(X_test)\n",
    "    \n",
    "    clf_report = classification_report(y_test, predicted)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, predicted)\n",
    "    print(clf_report, '\\n\\n', conf_matrix)\n",
    "    \n",
    "    return predicted, conf_matrix, clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:10:47.272602Z",
     "start_time": "2019-09-24T14:10:47.257642Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data.content,\n",
    "                                                    data.category,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:14:43.553064Z",
     "start_time": "2019-09-24T14:10:51.919560Z"
    }
   },
   "outputs": [],
   "source": [
    "bow_pred, bow_conf_matrix, bow_clf_report = classificate_with_bow(X_train_2, y_train_2, X_test_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T14:14:43.671747Z",
     "start_time": "2019-09-24T14:14:43.661774Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(bow_clf_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "189.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
