{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 1 : Claisficación de texto\n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:54:04.313032Z",
     "start_time": "2019-07-24T20:54:04.282368Z"
    }
   },
   "source": [
    "## Breve resumen de las clases anteriores\n",
    "\n",
    "\n",
    "### ¿En qué consiste la clasificación de texto?\n",
    "\n",
    "Según [wikipedia](https://en.wikipedia.org/wiki/Document_classification): \n",
    "\n",
    "    \"The task is to assign a document to one or more classes or categories\"\n",
    "\n",
    "\n",
    "Algunos ejemplos:\n",
    "\n",
    "- Assigning subject categories, topics, or genres\n",
    "- Spam detection \n",
    "- Authorship identification\n",
    "- Age/gender identification\n",
    "- Language Identification\n",
    "- Sentiment analysis\n",
    "- ...\n",
    "\n",
    "### Definición formal\n",
    "\n",
    "Input:\n",
    "- A document    $d$\n",
    "- A fixed set of classes $C=\\{c_{1},    c_{2},...,    c_{J}\\}$\n",
    "\n",
    "Output:    \n",
    "\n",
    "- A predicted class $c \\in C$\n",
    "\n",
    "### Tipos de técnicas de clasificación: \n",
    "\n",
    "- Hand-coded Rules. (No se verán en este aux).\n",
    "- Supervised Machine Learning: \n",
    "    - Naïve Bayes\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - k-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:59:04.618627Z",
     "start_time": "2019-07-24T20:59:04.602978Z"
    }
   },
   "source": [
    "## Objetivo del Auxiliar\n",
    "\n",
    "Introducirlos en los primeros tópicos y herramientas comunes de NLP.\n",
    "\n",
    "Para esto, implementaremos variados modelos de clasificación de texto destinadas a **predecir la categoría de noticias de la radio biobio**.\n",
    "\n",
    "Las tecnicas que usaremos serán las vistas en clases: \n",
    "\n",
    "- Bayes\n",
    "- Logistic regression \n",
    "\n",
    "Las herramientas que usaremos son (y qué serán **necesarias** para ejecutar este notebook): \n",
    "\n",
    "- Pandas\n",
    "- Scikit-Learn\n",
    "- Spacy\n",
    "- NLTK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créditos\n",
    "\n",
    "Todas las noticias extraidas perteneces a [Biobio Chile](https://www.biobiochile.cl/), los cuales gentilmente licencian todo su material a través de la [licencia Creative Commons (CC-BY-NC)](https://creativecommons.org/licenses/by-nc/2.0/cl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:03:52.553946Z",
     "start_time": "2019-07-24T21:03:52.538305Z"
    }
   },
   "source": [
    "## Referencias: \n",
    "\n",
    "Gitgub del curso: \n",
    "- https://github.com/dccuchile/CC6205\n",
    "\n",
    "Slides:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf\n",
    "\n",
    "\n",
    "Códigos varios:\n",
    "- https://affectivetweets.cms.waikato.ac.nz/benchmark/\n",
    "\n",
    "Algunos Recursos útiles\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Scikit-learn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "- [Spacy Tutorial](https://www.datacamp.com/community/blog/spacy-cheatsheet)\n",
    "- [NLTK Cheat sheet](http://sapir.psych.wisc.edu/programming_for_psychologists/cheat_sheets/Text-Analysis-with-NLTK-Cheatsheet.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:44:48.141367Z",
     "start_time": "2019-07-26T02:44:45.264797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=['ner', 'parser', 'tagger'])\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nacional = pd.read_json(\"./datasets/biobio_nacional.json\", encoding ='utf-8')\n",
    "internacional = pd.read_json(\"./datasets/biobio_internacional.json\", encoding ='utf-8')\n",
    "economia = pd.read_json(\"./datasets/biobio_economia.json\", encoding ='utf-8')\n",
    "sociedad = pd.read_json(\"./datasets/biobio_sociedad.json\", encoding ='utf-8')\n",
    "opinion = pd.read_json(\"./datasets/biobio_opinion.json\", encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_hour</th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>42</td>\n",
       "      <td>167</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>26/06/2019</td>\n",
       "      <td>15:12</td>\n",
       "      <td>César Vega Martínez</td>\n",
       "      <td>/lista/autores/cevega</td>\n",
       "      <td>Arqueólogos aseguran haber hallado el lugar ex...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/c...</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publication_date publication_hour               author  \\\n",
       "count               200              200                  200   \n",
       "unique               42              167                   13   \n",
       "top          26/06/2019            15:12  César Vega Martínez   \n",
       "freq                 10                3                   78   \n",
       "\n",
       "                  author_link  \\\n",
       "count                     200   \n",
       "unique                     13   \n",
       "top     /lista/autores/cevega   \n",
       "freq                       78   \n",
       "\n",
       "                                                    title  \\\n",
       "count                                                 200   \n",
       "unique                                                200   \n",
       "top     Arqueólogos aseguran haber hallado el lugar ex...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                     link  category  \\\n",
       "count                                                 200       200   \n",
       "unique                                                200         1   \n",
       "top     https://www.biobiochile.cl/noticias/sociedad/c...  Sociedad   \n",
       "freq                                                    1       200   \n",
       "\n",
       "       subcategory content tags embedded_links  \n",
       "count          200     200  200            200  \n",
       "unique           1     196  191              3  \n",
       "top       Sociedad           []             []  \n",
       "freq           200       5    9            198  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sociedad.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de noticia de categoría sociedad: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = sociedad.iloc[19:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_hour</th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>02/08/2019</td>\n",
       "      <td>15:08</td>\n",
       "      <td>Emilio Contreras</td>\n",
       "      <td>/lista/autores/Econtreras</td>\n",
       "      <td>Chile deja de utilizar 16.170 toneladas de bol...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/d...</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Chile dejó de utilizar 16.170 toneladas de b...</td>\n",
       "      <td>[#16.170 toneladas, #balance, #bolsas plástica...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publication_date publication_hour            author  \\\n",
       "19       02/08/2019            15:08  Emilio Contreras   \n",
       "\n",
       "                  author_link  \\\n",
       "19  /lista/autores/Econtreras   \n",
       "\n",
       "                                                title  \\\n",
       "19  Chile deja de utilizar 16.170 toneladas de bol...   \n",
       "\n",
       "                                                 link  category subcategory  \\\n",
       "19  https://www.biobiochile.cl/noticias/sociedad/d...  Sociedad    Sociedad   \n",
       "\n",
       "                                              content  \\\n",
       "19    Chile dejó de utilizar 16.170 toneladas de b...   \n",
       "\n",
       "                                                 tags embedded_links  \n",
       "19  [#16.170 toneladas, #balance, #bolsas plástica...             []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content = sample.values[0][8]\n",
    "sample_category = sample.values[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido:\n",
      "\n",
      " Chile dejó de utilizar 16.170 toneladas de bolsas plásticas desde que hace un año implementó una ley que prohíbe su entrega en supermercados y retails, informó este viernes el ministerio del Medio Ambiente.  Tras un periodo de prueba de seis meses, el gobierno chileno puso en vigencia en agosto del año pasado la ley que evitó el consumo de unas 2.200 millones de bolsas plásticas, una reducción significativa tomando en cuenta que, hasta la promulgación de la norma, Chile producía 3.200 millones de bolsas anuales .  “Si consideramos el peso de estas bolsas que se dejaron de entregar, unas 16.170 toneladas, equivalen a 13.940 autos”, indicó el ministerio en un comunicado.  Desde la puesta en marcha de la norma, los chilenos asumieron el hábito de utilizar bolsas reutilizables, de tela o material reciclable, lo cual ha colaborado en reducir la contaminación que producen los sacos plásticos principalmente en los océanos, en los que yacen unos 13 millones de toneladas de plásticos a nivel mundial.        Presentan proyecto para obligar al comercio a entregar gratis bolsas reciclables    La prohibición “genera un cambio de hábitos muy importante, algo que pensábamos que era imposible hacer en corto tiempo ya se ha hecho posible” , afirmó la ministra de Medio Ambiente, Carolina Schmidt , a medios locales.  Schimdt explicó que para el próximo año la ley se extenderá a pequeños almacenes y locales de barrio, que actualmente sólo pueden entregar dos bolsas por comprador.  Chile también se puso la meta de incrementar en un 60% el reciclaje de envases y embalajes de plásticos que en la actualidad alcanza al 12% .  Para 2025 , en tanto, el gobierno espera que el 100% de los envases de plásticos sean reutilizables o biodegradables, “un cambio radical” , según Schmidt.  Según la ONU, 5 billones de bolsas de plástico se consumen anualmente en el mundo, la mayoría hechas de polietileno, un derivado del petróleo que demora cerca de 500 años en biodegradarse. \n",
      "\n",
      "Clase:\n",
      "\n",
      " Sociedad\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mContenido:\\033[0m\\n\\n\", sample_content.strip(), \"\\n\\n\\033[1mClase:\\033[0m\\n\\n\", sample_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar\n",
    "\n",
    "¿Qué era tokenizar?\n",
    "\n",
    "    In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n",
    "    \n",
    "Referencia: [Tokenización en wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_content = [word.text for word in  nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dejó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utilizar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toneladas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plásticas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>desde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>año</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>implementó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>una</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>prohíbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>su</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>entrega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>supermercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>retails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>informó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>este</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>viernes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>plástico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>consumen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>anualmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>mayoría</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>hechas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>polietileno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>derivado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>petróleo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>demora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>cerca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>años</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>biodegradarse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0                 \n",
       "1            Chile\n",
       "2             dejó\n",
       "3               de\n",
       "4         utilizar\n",
       "..             ...\n",
       "366           años\n",
       "367             en\n",
       "368  biodegradarse\n",
       "369              .\n",
       "370               \n",
       "\n",
       "[371 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokenized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords \n",
    "\n",
    "¿Qué eran las stopwords?\n",
    "\n",
    "    In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Stop words are generally the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools avoid removing stop words to support phrase search. \n",
    "    \n",
    "Referencias: [Stopwords en Wikipedia](https://en.wikipedia.org/wiki/Stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>estaba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>podrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>estamos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>hizo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>los</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>ningunos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>hacen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>última</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>últimos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>añadió</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>fuimos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>poco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>sin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>excepto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>trabaja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>segun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aquel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>cuánto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>pocas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "127    estaba\n",
       "336        de\n",
       "272   podrian\n",
       "367   estamos\n",
       "160      hizo\n",
       "445       los\n",
       "340  ningunos\n",
       "314     hacen\n",
       "95     última\n",
       "449   últimos\n",
       "85     añadió\n",
       "295    fuimos\n",
       "395      poco\n",
       "263       sin\n",
       "90    excepto\n",
       "365   trabaja\n",
       "286     segun\n",
       "3       aquel\n",
       "48     cuánto\n",
       "234     pocas"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(STOP_WORDS).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_content_no_stop_words = [token for token in tokenized_content if token not in STOP_WORDS ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>toneladas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>plásticas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>importante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>polietileno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>año</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gobierno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>puso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>extenderá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>gratis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>obligar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>plástico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>barrio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>alcanza</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "105    toneladas\n",
       "64        16.170\n",
       "6      plásticas\n",
       "128   importante\n",
       "215  polietileno\n",
       "9            ley\n",
       "168            %\n",
       "111             \n",
       "148          año\n",
       "191            o\n",
       "27      gobierno\n",
       "164         puso\n",
       "150    extenderá\n",
       "118       gratis\n",
       "114      obligar\n",
       "93             ,\n",
       "101            ,\n",
       "208     plástico\n",
       "156       barrio\n",
       "175      alcanza"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokenized_content_no_stop_words).sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "¿Qué era el stemming? \n",
    "\n",
    "    Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n",
    "    \n",
    "Referencia: [Stemming en Wikipedia](https://en.wikipedia.org/wiki/Stemming)\n",
    "  \n",
    "#### Ejemplos: \n",
    "\n",
    "\n",
    "| word | stem of the word  |\n",
    "|---|---|\n",
    "working | work\n",
    "worked | work\n",
    "works | work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmed_content = [stemmer.stem(word) for word in tokenized_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>el</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ministerio</td>\n",
       "      <td>ministeri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>%</td>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>bolsas</td>\n",
       "      <td>bols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>“</td>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>envases</td>\n",
       "      <td>envas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>espera</td>\n",
       "      <td>esper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>producía</td>\n",
       "      <td>produc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>tela</td>\n",
       "      <td>tel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>ministra</td>\n",
       "      <td>ministr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>extenderá</td>\n",
       "      <td>extend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       original       stem\n",
       "260         que        que\n",
       "139          el         el\n",
       "31   ministerio  ministeri\n",
       "313           %          %\n",
       "40           de         de\n",
       "68       bolsas       bols\n",
       "95            “          “\n",
       "316     envases      envas\n",
       "292          en         en\n",
       "309      espera      esper\n",
       "87     producía     produc\n",
       "147        tela        tel\n",
       "204          de         de\n",
       "228    ministra    ministr\n",
       "251   extenderá     extend"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(tokenized_content, stemmed_content), columns=['original', 'stem']).sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "¿Qué era lematización? \n",
    "\n",
    "    \n",
    "    Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.[\n",
    "    \n",
    "    \n",
    "Referencia: [Lematización en wikipedia](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    \n",
    "#### Ejemplos\n",
    "\n",
    "| word | lemma  |\n",
    "|---|---|\n",
    "dije| decir \n",
    "guapas | guapo\n",
    "mesa | mesas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lemmatized_content = [word.lemma_ for word in nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>consumo</td>\n",
       "      <td>consumir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>la</td>\n",
       "      <td>lo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>“</td>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>del</td>\n",
       "      <td>del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>envases</td>\n",
       "      <td>envase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>informó</td>\n",
       "      <td>informar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>gobierno</td>\n",
       "      <td>gobernar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>agosto</td>\n",
       "      <td>agostar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>el</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>comprador</td>\n",
       "      <td>comprador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      original      lemma\n",
       "62     consumo   consumir\n",
       "197                      \n",
       "335         la         lo\n",
       "324          “          “\n",
       "359        del        del\n",
       "316    envases     envase\n",
       "27     informó   informar\n",
       "308   gobierno   gobernar\n",
       "3           de         de\n",
       "53      agosto    agostar\n",
       "120         el         el\n",
       "108          ,          ,\n",
       "364         de         de\n",
       "268  comprador  comprador\n",
       "347         en         en"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizar la lematización\n",
    "pd.DataFrame(zip(tokenized_content, lemmatized_content), columns=['original', 'lemma']).sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "¿Qué es?\n",
    "\n",
    "\n",
    "    \n",
    "    The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words \n",
    "    \n",
    "Referencia: [BoW en wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "- Doc1 : 'I love dogs'\n",
    "- Doc2: 'I hate dogs and knitting.\n",
    "- Doc3: 'Knitting is my hobby and my passion.\n",
    "\n",
    "![BOW](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg?resize=1024%2C260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers for CountVectorizer\n",
    "\n",
    "def tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc) ]\n",
    "    \n",
    "def tokenizer_with_stopwords(doc):\n",
    "    return [x.orth_ for x in nlp(doc) if x.orth_ not in STOP_WORDS]\n",
    "\n",
    "def tokenizer_with_lemmatization (doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]\n",
    "  \n",
    "def tokenizer_with_stemming(doc):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return [stemmer.stem(word) for word in [x.orth_ for x in nlp(doc)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2305 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 216 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', tokenizer = tokenizer, ngram_range=(1,2))  \n",
    "bow = vectorizer.fit_transform(opinion.sample(4).content)\n",
    "bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar los datasets\n",
    "\n",
    "Seleccionar solo las columnas relevantes y divider en conjuntos de entrenamiento y de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets(datasets):\n",
    "    dataset = pd.concat(datasets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.category, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [nacional, internacional, economia, sociedad, opinion]\n",
    "X_train, X_test, y_train, y_test = process_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de tópico con Naive Bayes\n",
    "\n",
    "- Simple (“naïve”) classification method based on Bayes rule\n",
    "- Relies on very simple representation of document\n",
    "- Bag of words\n",
    "\n",
    "Para un document d y la clase C, la probabilidad de está dada por:\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "Consideremos MAP como Maximum a posteriori o la clase mas probable. \n",
    "\n",
    "$$ C_{MAP} = argmax_{c \\in C} P(c|d)$$\n",
    "\n",
    "Aplicando el teorema de Bayes:\n",
    "\n",
    "$$ C_{MAP} = argmax_{c \\in C} \\frac{P(d|c)P(c)}{P(d)} $$\n",
    "\n",
    "Descartamos el denominador:\n",
    "\n",
    "$$ C_{MAP} = argmax_{c \\in C} P(d|c)P(c) $$\n",
    "\n",
    "Si el documento d, ahora lo consideramos como un arreglo de palabras (pensando en bag of words): \n",
    "\n",
    "$$ C_{MAP} = argmax_{c\\in C} P(x_1, x_2, ..., x_n | c)P(c) $$ \n",
    "\n",
    "El clasificador aprenderá $O(|X|^n * |C|)$ parámetros, los cuales deberán ser entrenados a partir de una gran cantidad de ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establecer el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer = TOKENIZER, ngram_range=(1,3))  \n",
    "clf = MultinomialNB()   \n",
    "\n",
    "text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x000001E085876158>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[56  0  2 11  1]\n",
      " [ 0 64  1  2 10]\n",
      " [ 1  0 37 19  1]\n",
      " [ 0  0  0 66  0]\n",
      " [ 0  5  0  3 51]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.98      0.80      0.88        70\n",
      "Internacional       0.93      0.83      0.88        77\n",
      "     Nacional       0.93      0.64      0.76        58\n",
      "      Opinion       0.65      1.00      0.79        66\n",
      "     Sociedad       0.81      0.86      0.84        59\n",
      "\n",
      "     accuracy                           0.83       330\n",
      "    macro avg       0.86      0.83      0.83       330\n",
      " weighted avg       0.86      0.83      0.83       330\n",
      "\n",
      "\n",
      "kappa:0.7873270881763988\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "kappa = cohen_kappa_score(y_test, predicted) \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nkappa:'+str(kappa))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype='<U13')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"En puerto montt se encontró un perrito, que aparentemente, habría consumido drogas de alto calibre. Producto de esto, padecera severa caña durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Internacional'], dtype='<U13')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"kim jong un será el próximo candidato a ministro de educación.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Economia'], dtype='<U13')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"El banco mundial presentó para chile un decrecimiento económico de 92% y una inflación de 8239832983289%.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T22:10:39.567162Z",
     "start_time": "2019-07-24T22:10:39.551542Z"
    }
   },
   "source": [
    "## Regresión Logísitica\n",
    "\n",
    "Explicación cuática aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x000001E085876158>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000,\n",
       "                                    multi_class='ovr', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[58  1  6  1  4]\n",
      " [ 5 61  5  1  5]\n",
      " [ 2  0 53  1  2]\n",
      " [ 1  1  2 61  1]\n",
      " [ 2  6  2  1 48]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.85      0.83      0.84        70\n",
      "Internacional       0.88      0.79      0.84        77\n",
      "     Nacional       0.78      0.91      0.84        58\n",
      "      Opinion       0.94      0.92      0.93        66\n",
      "     Sociedad       0.80      0.81      0.81        59\n",
      "\n",
      "     accuracy                           0.85       330\n",
      "    macro avg       0.85      0.85      0.85       330\n",
      " weighted avg       0.85      0.85      0.85       330\n",
      "\n",
      "\n",
      "kappa:0.8142510884174009\n"
     ]
    }
   ],
   "source": [
    "predicted = log_pipe.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "kappa = cohen_kappa_score(y_test, predicted) \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nkappa:'+str(kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.predict([\"En puerto montt se encontró un perrito, que aparentemente, habría consumido drogas de alto calibre. Producto de esto, padecera severa caña durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.predict([\"kim jong un será el próximo candidato a ministro de educación.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de Autoría\n",
    "\n",
    "¿Existirá un patrón en como escriben los periodistas que nos permitan identificarlos a partir de sus textos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gonzalo Cifuentes', 'Felipe Delgado', 'Nicolás Parra',\n",
       "       'Catalina Díaz', 'Valentina González', 'Emilio Lara',\n",
       "       'Manuel Stuardo', 'Nicolás Díaz', 'Sandar Oporto',\n",
       "       'María José Villarroel', 'Catalina Sánchez', 'Matías Vega',\n",
       "       'Manuel Cabrera', 'Periodismo UCSC', 'Diego Vera', 'Yerko Roa',\n",
       "       'Felipe Díaz Montero', 'Ariela Muñoz', 'Yessenia Márquez',\n",
       "       'Gerson Guzmán D.', 'Paola Alemán', 'Sebastián Asencio',\n",
       "       'Claudia Miño', 'Camilo Suazo', 'Verónica Reyes', 'Max Duhalde',\n",
       "       'Francisca Rivas', 'Hernán Bustamante', 'Leonardo Casas',\n",
       "       'Alberto González', 'Jonathan Flores', 'Scarlet Stuardo',\n",
       "       'Gerson Guzmán', 'Bernardita Villa', 'César Vega Martínez',\n",
       "       'Camila Álvarez', 'Jaime Parra', 'Emilio Contreras',\n",
       "       'Fabián Barría', 'Denisse Charpentier', 'Nicole Briones', 'Tu Voz',\n",
       "       'Natalia Muñoz', 'Tamara Rojas', 'Alejandra Soto', 'Pablo Cabeza'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(datasets).author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets_by_author(datasets):\n",
    "    dataset = pd.concat(datasets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.author, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = process_datasets_by_author(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod_by_author = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe_by_author = Pipeline([('vect', vectorizer), ('clf', log_mod_by_author)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x000001E085876158>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000,\n",
       "                                    multi_class='ovr', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe_by_author.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  1 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  1 ... 34  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      "Classification Report\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Alejandra Soto       0.00      0.00      0.00         3\n",
      "         Ariela Muñoz       0.00      0.00      0.00         3\n",
      "     Bernardita Villa       0.12      0.25      0.17         4\n",
      "       Camila Álvarez       0.00      0.00      0.00         1\n",
      "         Camilo Suazo       0.50      0.11      0.18         9\n",
      "        Catalina Díaz       0.00      0.00      0.00         3\n",
      "         Claudia Miño       0.00      0.00      0.00         3\n",
      "  César Vega Martínez       0.70      0.76      0.73        25\n",
      "  Denisse Charpentier       0.00      0.00      0.00         1\n",
      "           Diego Vera       0.45      0.75      0.56        44\n",
      "     Emilio Contreras       0.00      0.00      0.00         2\n",
      "          Emilio Lara       0.40      0.17      0.24        12\n",
      "       Felipe Delgado       0.13      0.40      0.20         5\n",
      "  Felipe Díaz Montero       0.25      0.33      0.29         3\n",
      "      Francisca Rivas       0.00      0.00      0.00         7\n",
      "        Gerson Guzmán       0.00      0.00      0.00         1\n",
      "     Gerson Guzmán D.       0.00      0.00      0.00         2\n",
      "    Gonzalo Cifuentes       0.00      0.00      0.00         8\n",
      "    Hernán Bustamante       0.00      0.00      0.00         2\n",
      "      Jonathan Flores       0.00      0.00      0.00         4\n",
      "       Manuel Cabrera       0.33      0.10      0.15        10\n",
      "       Manuel Stuardo       0.00      0.00      0.00         2\n",
      "María José Villarroel       0.71      0.89      0.79        36\n",
      "          Matías Vega       0.18      0.15      0.17        13\n",
      "          Max Duhalde       0.00      0.00      0.00         1\n",
      "       Nicole Briones       0.00      0.00      0.00         1\n",
      "         Nicolás Díaz       0.00      0.00      0.00         6\n",
      "        Nicolás Parra       0.00      0.00      0.00         8\n",
      "         Paola Alemán       1.00      0.25      0.40         4\n",
      "        Sandar Oporto       0.50      0.10      0.17        10\n",
      "      Scarlet Stuardo       0.75      0.80      0.77        15\n",
      "    Sebastián Asencio       0.00      0.00      0.00         2\n",
      "         Tamara Rojas       0.00      0.00      0.00         5\n",
      "               Tu Voz       0.89      0.73      0.80        11\n",
      "   Valentina González       0.39      0.62      0.48        21\n",
      "       Verónica Reyes       0.55      0.85      0.67        40\n",
      "            Yerko Roa       0.00      0.00      0.00         1\n",
      "     Yessenia Márquez       0.00      0.00      0.00         2\n",
      "\n",
      "             accuracy                           0.49       330\n",
      "            macro avg       0.21      0.19      0.18       330\n",
      "         weighted avg       0.42      0.49      0.43       330\n",
      "\n",
      "\n",
      "kappa:0.4455288151963941\n"
     ]
    }
   ],
   "source": [
    "predicted = log_pipe_by_author.predict(X_test_2)\n",
    "\n",
    "conf = confusion_matrix(y_test_2, predicted)\n",
    "kappa = cohen_kappa_score(y_test_2, predicted) \n",
    "class_rep = classification_report(y_test_2, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nkappa:'+str(kappa))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
