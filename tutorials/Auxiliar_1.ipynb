{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 1 : Claisficación de texto\n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:54:04.313032Z",
     "start_time": "2019-07-24T20:54:04.282368Z"
    }
   },
   "source": [
    "## Breve resumen de las clases anteriores\n",
    "\n",
    "\n",
    "### ¿En qué consiste la clasificación de texto?\n",
    "\n",
    "Según [wikipedia](https://en.wikipedia.org/wiki/Document_classification): \n",
    "\n",
    "    \"The task is to assign a document to one or more classes or categories\"\n",
    "\n",
    "\n",
    "Algunos ejemplos:\n",
    "\n",
    "- Assigning subject categories, topics, or genres\n",
    "- Spam detection \n",
    "- Authorship identification\n",
    "- Age/gender identification\n",
    "- Language Identification\n",
    "- Sentiment analysis\n",
    "- ...\n",
    "\n",
    "### Definición formal\n",
    "\n",
    "Input:\n",
    "- A document    $d$\n",
    "- A fixed set of classes $C=\\{c_{1},    c_{2},...,    c_{J}\\}$\n",
    "\n",
    "Output:    \n",
    "\n",
    "- A predicted class $c \\in C$\n",
    "\n",
    "### Tipos de técnicas de clasificación: \n",
    "\n",
    "- Hand-coded Rules. (No se verán en este aux).\n",
    "- Supervised Machine Learning: \n",
    "    - Naïve Bayes\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - k-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:59:04.618627Z",
     "start_time": "2019-07-24T20:59:04.602978Z"
    }
   },
   "source": [
    "## Objetivo del Auxiliar\n",
    "\n",
    "Introducirlos en los primeros tópicos y herramientas comunes de NLP.\n",
    "\n",
    "Para esto, implementaremos variados modelos de clasificación de texto destinadas a **predecir la categoría de noticias de la radio biobio**.\n",
    "\n",
    "Las tecnicas que usaremos serán las vistas en clases: \n",
    "\n",
    "- Bayes\n",
    "- Logistic regression \n",
    "\n",
    "Las herramientas que usaremos son (y qué serán **necesarias** para ejecutar este notebook): \n",
    "\n",
    "- Pandas\n",
    "- Scikit-Learn\n",
    "- Spacy\n",
    "- NLTK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créditos\n",
    "\n",
    "Todas las noticias extraidas perteneces a [Biobio Chile](https://www.biobiochile.cl/), los cuales gentilmente licencian todo su material a través de la [licencia Creative Commons (CC-BY-NC)](https://creativecommons.org/licenses/by-nc/2.0/cl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T21:03:52.553946Z",
     "start_time": "2019-07-24T21:03:52.538305Z"
    }
   },
   "source": [
    "## Referencias: \n",
    "\n",
    "Gitgub del curso: \n",
    "- https://github.com/dccuchile/CC6205\n",
    "\n",
    "Slides:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf\n",
    "\n",
    "\n",
    "Códigos varios:\n",
    "- https://affectivetweets.cms.waikato.ac.nz/benchmark/\n",
    "\n",
    "Algunos Recursos útiles\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Scikit-learn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "- [Spacy Tutorial](https://www.datacamp.com/community/blog/spacy-cheatsheet)\n",
    "- [NLTK Cheat sheet](http://sapir.psych.wisc.edu/programming_for_psychologists/cheat_sheets/Text-Analysis-with-NLTK-Cheatsheet.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T02:44:48.141367Z",
     "start_time": "2019-07-26T02:44:45.264797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=['ner', 'parser', 'tagger'])\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nacional = pd.read_json(\"./datasets/biobio_nacional.json\", encoding ='utf-8')\n",
    "internacional = pd.read_json(\"./datasets/biobio_internacional.json\", encoding ='utf-8')\n",
    "economia = pd.read_json(\"./datasets/biobio_economia.json\", encoding ='utf-8')\n",
    "sociedad = pd.read_json(\"./datasets/biobio_sociedad.json\", encoding ='utf-8')\n",
    "opinion = pd.read_json(\"./datasets/biobio_opinion.json\", encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_hour</th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>42</td>\n",
       "      <td>167</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>26/06/2019</td>\n",
       "      <td>15:12</td>\n",
       "      <td>César Vega Martínez</td>\n",
       "      <td>/lista/autores/cevega</td>\n",
       "      <td>Hombre irrumpe de noche en estación de policía...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/m...</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publication_date publication_hour               author  \\\n",
       "count               200              200                  200   \n",
       "unique               42              167                   13   \n",
       "top          26/06/2019            15:12  César Vega Martínez   \n",
       "freq                 10                3                   78   \n",
       "\n",
       "                  author_link  \\\n",
       "count                     200   \n",
       "unique                     13   \n",
       "top     /lista/autores/cevega   \n",
       "freq                       78   \n",
       "\n",
       "                                                    title  \\\n",
       "count                                                 200   \n",
       "unique                                                200   \n",
       "top     Hombre irrumpe de noche en estación de policía...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                     link  category  \\\n",
       "count                                                 200       200   \n",
       "unique                                                200         1   \n",
       "top     https://www.biobiochile.cl/noticias/sociedad/m...  Sociedad   \n",
       "freq                                                    1       200   \n",
       "\n",
       "       subcategory content tags embedded_links  \n",
       "count          200     200  200            200  \n",
       "unique           1     196  191              3  \n",
       "top       Sociedad           []             []  \n",
       "freq           200       5    9            198  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sociedad.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de noticia de categoría sociedad: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = sociedad.iloc[19:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_hour</th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>02/08/2019</td>\n",
       "      <td>15:08</td>\n",
       "      <td>Emilio Contreras</td>\n",
       "      <td>/lista/autores/Econtreras</td>\n",
       "      <td>Chile deja de utilizar 16.170 toneladas de bol...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/d...</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Chile dejó de utilizar 16.170 toneladas de b...</td>\n",
       "      <td>[#16.170 toneladas, #balance, #bolsas plástica...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publication_date publication_hour            author  \\\n",
       "19       02/08/2019            15:08  Emilio Contreras   \n",
       "\n",
       "                  author_link  \\\n",
       "19  /lista/autores/Econtreras   \n",
       "\n",
       "                                                title  \\\n",
       "19  Chile deja de utilizar 16.170 toneladas de bol...   \n",
       "\n",
       "                                                 link  category subcategory  \\\n",
       "19  https://www.biobiochile.cl/noticias/sociedad/d...  Sociedad    Sociedad   \n",
       "\n",
       "                                              content  \\\n",
       "19    Chile dejó de utilizar 16.170 toneladas de b...   \n",
       "\n",
       "                                                 tags embedded_links  \n",
       "19  [#16.170 toneladas, #balance, #bolsas plástica...             []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content = sample.values[0][8]\n",
    "sample_category = sample.values[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido:\n",
      "\n",
      " Chile dejó de utilizar 16.170 toneladas de bolsas plásticas desde que hace un año implementó una ley que prohíbe su entrega en supermercados y retails, informó este viernes el ministerio del Medio Ambiente.  Tras un periodo de prueba de seis meses, el gobierno chileno puso en vigencia en agosto del año pasado la ley que evitó el consumo de unas 2.200 millones de bolsas plásticas, una reducción significativa tomando en cuenta que, hasta la promulgación de la norma, Chile producía 3.200 millones de bolsas anuales .  “Si consideramos el peso de estas bolsas que se dejaron de entregar, unas 16.170 toneladas, equivalen a 13.940 autos”, indicó el ministerio en un comunicado.  Desde la puesta en marcha de la norma, los chilenos asumieron el hábito de utilizar bolsas reutilizables, de tela o material reciclable, lo cual ha colaborado en reducir la contaminación que producen los sacos plásticos principalmente en los océanos, en los que yacen unos 13 millones de toneladas de plásticos a nivel mundial.        Presentan proyecto para obligar al comercio a entregar gratis bolsas reciclables    La prohibición “genera un cambio de hábitos muy importante, algo que pensábamos que era imposible hacer en corto tiempo ya se ha hecho posible” , afirmó la ministra de Medio Ambiente, Carolina Schmidt , a medios locales.  Schimdt explicó que para el próximo año la ley se extenderá a pequeños almacenes y locales de barrio, que actualmente sólo pueden entregar dos bolsas por comprador.  Chile también se puso la meta de incrementar en un 60% el reciclaje de envases y embalajes de plásticos que en la actualidad alcanza al 12% .  Para 2025 , en tanto, el gobierno espera que el 100% de los envases de plásticos sean reutilizables o biodegradables, “un cambio radical” , según Schmidt.  Según la ONU, 5 billones de bolsas de plástico se consumen anualmente en el mundo, la mayoría hechas de polietileno, un derivado del petróleo que demora cerca de 500 años en biodegradarse. \n",
      "\n",
      "Clase:\n",
      "\n",
      " Sociedad\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mContenido:\\033[0m\\n\\n\", sample_content.strip(), \"\\n\\n\\033[1mClase:\\033[0m\\n\\n\", sample_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar\n",
    "\n",
    "¿Qué era tokenizar?\n",
    "\n",
    "    In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n",
    "    \n",
    "Referencia: [Tokenización en wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_content = [word.text for word in  nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dejó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utilizar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toneladas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plásticas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>desde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>año</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>implementó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>una</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>prohíbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>su</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>entrega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>supermercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>retails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>informó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>este</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>viernes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>plástico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>consumen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>anualmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>mayoría</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>hechas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>polietileno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>derivado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>petróleo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>demora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>cerca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>años</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>biodegradarse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0                 \n",
       "1            Chile\n",
       "2             dejó\n",
       "3               de\n",
       "4         utilizar\n",
       "..             ...\n",
       "366           años\n",
       "367             en\n",
       "368  biodegradarse\n",
       "369              .\n",
       "370               \n",
       "\n",
       "[371 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokenized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords \n",
    "\n",
    "¿Qué eran las stopwords?\n",
    "\n",
    "    In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Stop words are generally the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools avoid removing stop words to support phrase search. \n",
    "    \n",
    "Referencias: [Stopwords en Wikipedia](https://en.wikipedia.org/wiki/Stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actualmente',\n",
       " 'acuerdo',\n",
       " 'adelante',\n",
       " 'ademas',\n",
       " 'además',\n",
       " 'adrede',\n",
       " 'afirmó',\n",
       " 'agregó',\n",
       " 'ahi',\n",
       " 'ahora',\n",
       " 'ahí',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'alguna',\n",
       " 'algunas',\n",
       " 'alguno',\n",
       " 'algunos',\n",
       " 'algún',\n",
       " 'alli',\n",
       " 'allí',\n",
       " 'alrededor',\n",
       " 'ambos',\n",
       " 'ampleamos',\n",
       " 'antano',\n",
       " 'antaño',\n",
       " 'ante',\n",
       " 'anterior',\n",
       " 'antes',\n",
       " 'apenas',\n",
       " 'aproximadamente',\n",
       " 'aquel',\n",
       " 'aquella',\n",
       " 'aquellas',\n",
       " 'aquello',\n",
       " 'aquellos',\n",
       " 'aqui',\n",
       " 'aquél',\n",
       " 'aquélla',\n",
       " 'aquéllas',\n",
       " 'aquéllos',\n",
       " 'aquí',\n",
       " 'arriba',\n",
       " 'arribaabajo',\n",
       " 'aseguró',\n",
       " 'asi',\n",
       " 'así',\n",
       " 'atras',\n",
       " 'aun',\n",
       " 'aunque',\n",
       " 'ayer',\n",
       " 'añadió',\n",
       " 'aún',\n",
       " 'bajo',\n",
       " 'bastante',\n",
       " 'bien',\n",
       " 'breve',\n",
       " 'buen',\n",
       " 'buena',\n",
       " 'buenas',\n",
       " 'bueno',\n",
       " 'buenos',\n",
       " 'cada',\n",
       " 'casi',\n",
       " 'cerca',\n",
       " 'cierta',\n",
       " 'ciertas',\n",
       " 'cierto',\n",
       " 'ciertos',\n",
       " 'cinco',\n",
       " 'claro',\n",
       " 'comentó',\n",
       " 'como',\n",
       " 'con',\n",
       " 'conmigo',\n",
       " 'conocer',\n",
       " 'conseguimos',\n",
       " 'conseguir',\n",
       " 'considera',\n",
       " 'consideró',\n",
       " 'consigo',\n",
       " 'consigue',\n",
       " 'consiguen',\n",
       " 'consigues',\n",
       " 'contigo',\n",
       " 'contra',\n",
       " 'cosas',\n",
       " 'creo',\n",
       " 'cual',\n",
       " 'cuales',\n",
       " 'cualquier',\n",
       " 'cuando',\n",
       " 'cuanta',\n",
       " 'cuantas',\n",
       " 'cuanto',\n",
       " 'cuantos',\n",
       " 'cuatro',\n",
       " 'cuenta',\n",
       " 'cuál',\n",
       " 'cuáles',\n",
       " 'cuándo',\n",
       " 'cuánta',\n",
       " 'cuántas',\n",
       " 'cuánto',\n",
       " 'cuántos',\n",
       " 'cómo',\n",
       " 'da',\n",
       " 'dado',\n",
       " 'dan',\n",
       " 'dar',\n",
       " 'de',\n",
       " 'debajo',\n",
       " 'debe',\n",
       " 'deben',\n",
       " 'debido',\n",
       " 'decir',\n",
       " 'dejó',\n",
       " 'del',\n",
       " 'delante',\n",
       " 'demasiado',\n",
       " 'demás',\n",
       " 'dentro',\n",
       " 'deprisa',\n",
       " 'desde',\n",
       " 'despacio',\n",
       " 'despues',\n",
       " 'después',\n",
       " 'detras',\n",
       " 'detrás',\n",
       " 'dia',\n",
       " 'dias',\n",
       " 'dice',\n",
       " 'dicen',\n",
       " 'dicho',\n",
       " 'dieron',\n",
       " 'diferente',\n",
       " 'diferentes',\n",
       " 'dijeron',\n",
       " 'dijo',\n",
       " 'dio',\n",
       " 'donde',\n",
       " 'dos',\n",
       " 'durante',\n",
       " 'día',\n",
       " 'días',\n",
       " 'dónde',\n",
       " 'ejemplo',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ello',\n",
       " 'ellos',\n",
       " 'embargo',\n",
       " 'empleais',\n",
       " 'emplean',\n",
       " 'emplear',\n",
       " 'empleas',\n",
       " 'empleo',\n",
       " 'en',\n",
       " 'encima',\n",
       " 'encuentra',\n",
       " 'enfrente',\n",
       " 'enseguida',\n",
       " 'entonces',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eramos',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estaban',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estais',\n",
       " 'estamos',\n",
       " 'estan',\n",
       " 'estar',\n",
       " 'estará',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'están',\n",
       " 'ex',\n",
       " 'excepto',\n",
       " 'existe',\n",
       " 'existen',\n",
       " 'explicó',\n",
       " 'expresó',\n",
       " 'fin',\n",
       " 'final',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fueron',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'general',\n",
       " 'gran',\n",
       " 'grandes',\n",
       " 'gueno',\n",
       " 'ha',\n",
       " 'haber',\n",
       " 'habia',\n",
       " 'habla',\n",
       " 'hablan',\n",
       " 'habrá',\n",
       " 'había',\n",
       " 'habían',\n",
       " 'hace',\n",
       " 'haceis',\n",
       " 'hacemos',\n",
       " 'hacen',\n",
       " 'hacer',\n",
       " 'hacerlo',\n",
       " 'haces',\n",
       " 'hacia',\n",
       " 'haciendo',\n",
       " 'hago',\n",
       " 'han',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'he',\n",
       " 'hecho',\n",
       " 'hemos',\n",
       " 'hicieron',\n",
       " 'hizo',\n",
       " 'horas',\n",
       " 'hoy',\n",
       " 'hubo',\n",
       " 'igual',\n",
       " 'incluso',\n",
       " 'indicó',\n",
       " 'informo',\n",
       " 'informó',\n",
       " 'intenta',\n",
       " 'intentais',\n",
       " 'intentamos',\n",
       " 'intentan',\n",
       " 'intentar',\n",
       " 'intentas',\n",
       " 'intento',\n",
       " 'ir',\n",
       " 'junto',\n",
       " 'la',\n",
       " 'lado',\n",
       " 'largo',\n",
       " 'las',\n",
       " 'le',\n",
       " 'lejos',\n",
       " 'les',\n",
       " 'llegó',\n",
       " 'lleva',\n",
       " 'llevar',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'luego',\n",
       " 'lugar',\n",
       " 'mal',\n",
       " 'manera',\n",
       " 'manifestó',\n",
       " 'mas',\n",
       " 'mayor',\n",
       " 'me',\n",
       " 'mediante',\n",
       " 'medio',\n",
       " 'mejor',\n",
       " 'mencionó',\n",
       " 'menos',\n",
       " 'menudo',\n",
       " 'mi',\n",
       " 'mia',\n",
       " 'mias',\n",
       " 'mientras',\n",
       " 'mio',\n",
       " 'mios',\n",
       " 'mis',\n",
       " 'misma',\n",
       " 'mismas',\n",
       " 'mismo',\n",
       " 'mismos',\n",
       " 'modo',\n",
       " 'momento',\n",
       " 'mucha',\n",
       " 'muchas',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'nadie',\n",
       " 'ni',\n",
       " 'ninguna',\n",
       " 'ningunas',\n",
       " 'ninguno',\n",
       " 'ningunos',\n",
       " 'ningún',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'nueva',\n",
       " 'nuevas',\n",
       " 'nuevo',\n",
       " 'nuevos',\n",
       " 'nunca',\n",
       " 'ocho',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'pais',\n",
       " 'para',\n",
       " 'parece',\n",
       " 'parte',\n",
       " 'partir',\n",
       " 'pasada',\n",
       " 'pasado',\n",
       " 'paìs',\n",
       " 'peor',\n",
       " 'pero',\n",
       " 'pesar',\n",
       " 'poca',\n",
       " 'pocas',\n",
       " 'poco',\n",
       " 'pocos',\n",
       " 'podeis',\n",
       " 'podemos',\n",
       " 'poder',\n",
       " 'podria',\n",
       " 'podriais',\n",
       " 'podriamos',\n",
       " 'podrian',\n",
       " 'podrias',\n",
       " 'podrá',\n",
       " 'podrán',\n",
       " 'podría',\n",
       " 'podrían',\n",
       " 'poner',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'posible',\n",
       " 'primer',\n",
       " 'primera',\n",
       " 'primero',\n",
       " 'primeros',\n",
       " 'principalmente',\n",
       " 'pronto',\n",
       " 'propia',\n",
       " 'propias',\n",
       " 'propio',\n",
       " 'propios',\n",
       " 'proximo',\n",
       " 'próximo',\n",
       " 'próximos',\n",
       " 'pudo',\n",
       " 'pueda',\n",
       " 'puede',\n",
       " 'pueden',\n",
       " 'puedo',\n",
       " 'pues',\n",
       " 'qeu',\n",
       " 'que',\n",
       " 'quedó',\n",
       " 'queremos',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'quiere',\n",
       " 'quiza',\n",
       " 'quizas',\n",
       " 'quizá',\n",
       " 'quizás',\n",
       " 'quién',\n",
       " 'quiénes',\n",
       " 'qué',\n",
       " 'raras',\n",
       " 'realizado',\n",
       " 'realizar',\n",
       " 'realizó',\n",
       " 'repente',\n",
       " 'respecto',\n",
       " 'sabe',\n",
       " 'sabeis',\n",
       " 'sabemos',\n",
       " 'saben',\n",
       " 'saber',\n",
       " 'sabes',\n",
       " 'salvo',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'sean',\n",
       " 'segun',\n",
       " 'segunda',\n",
       " 'segundo',\n",
       " 'según',\n",
       " 'seis',\n",
       " 'ser',\n",
       " 'sera',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'señaló',\n",
       " 'si',\n",
       " 'sido',\n",
       " 'siempre',\n",
       " 'siendo',\n",
       " 'siete',\n",
       " 'sigue',\n",
       " 'siguiente',\n",
       " 'sin',\n",
       " 'sino',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'sola',\n",
       " 'solamente',\n",
       " 'solas',\n",
       " 'solo',\n",
       " 'solos',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'soyos',\n",
       " 'su',\n",
       " 'supuesto',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'sé',\n",
       " 'sí',\n",
       " 'sólo',\n",
       " 'tal',\n",
       " 'tambien',\n",
       " 'también',\n",
       " 'tampoco',\n",
       " 'tan',\n",
       " 'tanto',\n",
       " 'tarde',\n",
       " 'te',\n",
       " 'temprano',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'teneis',\n",
       " 'tenemos',\n",
       " 'tener',\n",
       " 'tenga',\n",
       " 'tengo',\n",
       " 'tenido',\n",
       " 'tenía',\n",
       " 'tercera',\n",
       " 'ti',\n",
       " 'tiempo',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'toda',\n",
       " 'todas',\n",
       " 'todavia',\n",
       " 'todavía',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'total',\n",
       " 'trabaja',\n",
       " 'trabajais',\n",
       " 'trabajamos',\n",
       " 'trabajan',\n",
       " 'trabajar',\n",
       " 'trabajas',\n",
       " 'trabajo',\n",
       " 'tras',\n",
       " 'trata',\n",
       " 'través',\n",
       " 'tres',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'ultimo',\n",
       " 'un',\n",
       " 'una',\n",
       " 'unas',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'usa',\n",
       " 'usais',\n",
       " 'usamos',\n",
       " 'usan',\n",
       " 'usar',\n",
       " 'usas',\n",
       " 'uso',\n",
       " 'usted',\n",
       " 'ustedes',\n",
       " 'va',\n",
       " 'vais',\n",
       " 'valor',\n",
       " 'vamos',\n",
       " 'van',\n",
       " 'varias',\n",
       " 'varios',\n",
       " 'vaya',\n",
       " 'veces',\n",
       " 'ver',\n",
       " 'verdad',\n",
       " 'verdadera',\n",
       " 'verdadero',\n",
       " 'vez',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'voy',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'ésa',\n",
       " 'ésas',\n",
       " 'ése',\n",
       " 'ésos',\n",
       " 'ésta',\n",
       " 'éstas',\n",
       " 'éste',\n",
       " 'éstos',\n",
       " 'última',\n",
       " 'últimas',\n",
       " 'último',\n",
       " 'últimos'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_content_no_stop_words = [token for token in tokenized_content if token not in STOP_WORDS ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utilizar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toneladas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>plásticas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>año</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>implementó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prohíbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>entrega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>supermercados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>retails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>viernes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ministerio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Medio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ambiente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>periodo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>prueba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>meses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gobierno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>chileno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>puso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>cambio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>radical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Schmidt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Según</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>ONU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>billones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>plástico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>consumen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>anualmente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>mayoría</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>hechas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>polietileno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>derivado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>petróleo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>demora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>años</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>biodegradarse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0                 \n",
       "1            Chile\n",
       "2         utilizar\n",
       "3           16.170\n",
       "4        toneladas\n",
       "..             ...\n",
       "220            500\n",
       "221           años\n",
       "222  biodegradarse\n",
       "223              .\n",
       "224               \n",
       "\n",
       "[225 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokenized_content_no_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "¿Qué era el stemming? \n",
    "\n",
    "    Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n",
    "    \n",
    "Referencia: [Stemming en Wikipedia](https://en.wikipedia.org/wiki/Stemming)\n",
    "  \n",
    "#### Ejemplos: \n",
    "\n",
    "\n",
    "| word | stem of the word  |\n",
    "|---|---|\n",
    "working | work\n",
    "worked | work\n",
    "works | work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmed_content = [stemmer.stem(word) for word in tokenized_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>el</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>pequeños</td>\n",
       "      <td>pequeñ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>del</td>\n",
       "      <td>del</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2025</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>viernes</td>\n",
       "      <td>viern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>plásticos</td>\n",
       "      <td>plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>lo</td>\n",
       "      <td>lo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>muy</td>\n",
       "      <td>muy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      original     stem\n",
       "98          el       el\n",
       "253   pequeños   pequeñ\n",
       "54         del      del\n",
       "302       2025     2025\n",
       "135          ,        ,\n",
       "146         de       de\n",
       "29     viernes    viern\n",
       "304         en       en\n",
       "350          ,        ,\n",
       "285         de       de\n",
       "180  plásticos  plastic\n",
       "172        que      que\n",
       "152         lo       lo\n",
       "206        muy      muy\n",
       "270                    "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(tokenized_content, stemmed_content), columns=['original', 'stem']).sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "¿Qué era lematización? \n",
    "\n",
    "    \n",
    "    Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.[\n",
    "    \n",
    "    \n",
    "Referencia: [Lematización en wikipedia](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    \n",
    "#### Ejemplos\n",
    "\n",
    "| word | lemma  |\n",
    "|---|---|\n",
    "dije| decir \n",
    "guapas | guapo\n",
    "mesa | mesas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lemmatized_content = [word.lemma_ for word in nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>una</td>\n",
       "      <td>uno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>colaborado</td>\n",
       "      <td>colaborar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>3.200</td>\n",
       "      <td>3.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>gratis</td>\n",
       "      <td>gratis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>un</td>\n",
       "      <td>uno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>unas</td>\n",
       "      <td>uno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>hacer</td>\n",
       "      <td>hacer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>la</td>\n",
       "      <td>lo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>la</td>\n",
       "      <td>lo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       original      lemma\n",
       "71          una        uno\n",
       "216          en         en\n",
       "146          de         de\n",
       "155  colaborado  colaborar\n",
       "197                       \n",
       "88        3.200      3.200\n",
       "194      gratis     gratis\n",
       "123          un        uno\n",
       "109        unas        uno\n",
       "215       hacer      hacer\n",
       "128          la         lo\n",
       "141          de         de\n",
       "151           ,          ,\n",
       "7            de         de\n",
       "248          la         lo"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizar la lematización\n",
    "pd.DataFrame(zip(tokenized_content, lemmatized_content), columns=['original', 'lemma']).sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "¿Qué es?\n",
    "\n",
    "\n",
    "    \n",
    "    The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words \n",
    "    \n",
    "Referencia: [BoW en wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "- Doc1 : 'I love dogs'\n",
    "- Doc2: 'I hate dogs and knitting.\n",
    "- Doc3: 'Knitting is my hobby and my passion.\n",
    "\n",
    "![BOW](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg?resize=1024%2C260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers for CountVectorizer\n",
    "\n",
    "def tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc) ]\n",
    "    \n",
    "def tokenizer_with_stopwords(doc):\n",
    "    return [x.orth_ for x in nlp(doc) if x.orth_ not in STOP_WORDS]\n",
    "\n",
    "def tokenizer_with_lemmatization (doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]\n",
    "  \n",
    "def tokenizer_with_stemming(doc):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return [stemmer.stem(word) for word in [x.orth_ for x in nlp(doc)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2023 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 656 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', tokenizer = tokenizer, ngram_range=(1,2))  \n",
    "bow = vectorizer.fit_transform(opinion.sample(4).content)\n",
    "bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar los datasets\n",
    "\n",
    "Seleccionar solo las columnas relevantes y divider en conjuntos de entrenamiento y de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets(datasets):\n",
    "    dataset = pd.concat(datasets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.category, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [nacional, internacional, economia, sociedad, opinion]\n",
    "X_train, X_test, y_train, y_test = process_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con Naive Bayes\n",
    "\n",
    "- Simple (“naïve”) classification method based on Bayes rule\n",
    "- Relies on very simple representation of document\n",
    "- Bag of words\n",
    "\n",
    "Para un document d y la clase C, la probabilidad de está dada por:\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "Consideremos MAP como Maximum a posteriori o la clase mas probable. \n",
    "\n",
    "$$ C_{MAP} = argmax_{c \\in C} P(c|d)$$\n",
    "\n",
    "Aplicando el teorema de Bayes:\n",
    "\n",
    "$$ C_{MAP} = argmax_{c \\in C} \\frac{P(d|c)P(c)}{P(d)} $$\n",
    "\n",
    "Descartamos el denominador:\n",
    "\n",
    "$$ C_{MAP} = argmax_{c \\in C} P(d|c)P(c) $$\n",
    "\n",
    "Si el documento d, ahora lo consideramos como un arreglo de palabras (pensando en bag of words): \n",
    "\n",
    "$$ C_{MAP} = argmax_{c\\in C} P(x_1, x_2, ..., x_n | c)P(c) $$ \n",
    "\n",
    "El clasificador aprenderá $O(|X|^n * |C|)$ parámetros, los cuales deberán ser entrenados a partir de una gran cantidad de ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establecer el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer = TOKENIZER, ngram_range=(1,3))  \n",
    "clf = MultinomialNB()   \n",
    "\n",
    "text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x00000232CE393840>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[56  0  2 11  1]\n",
      " [ 0 64  1  2 10]\n",
      " [ 1  0 37 19  1]\n",
      " [ 0  0  0 66  0]\n",
      " [ 0  5  0  3 51]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.98      0.80      0.88        70\n",
      "Internacional       0.93      0.83      0.88        77\n",
      "     Nacional       0.93      0.64      0.76        58\n",
      "      Opinion       0.65      1.00      0.79        66\n",
      "     Sociedad       0.81      0.86      0.84        59\n",
      "\n",
      "     accuracy                           0.83       330\n",
      "    macro avg       0.86      0.83      0.83       330\n",
      " weighted avg       0.86      0.83      0.83       330\n",
      "\n",
      "\n",
      "kappa:0.7873270881763988\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "kappa = cohen_kappa_score(y_test, predicted) \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nkappa:'+str(kappa))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype='<U13')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"En puerto montt se encontró un perrito, que aparentemente, habría consumido drogas de alto calibre. Producto de esto, padecera severa caña durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Internacional'], dtype='<U13')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"kim jong un será el próximo candidato a ministro de educación.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Economia'], dtype='<U13')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"El banco mundial presentó para chile un decrecimiento económico de 92% y una inflación de 8239832983289%.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T22:10:39.567162Z",
     "start_time": "2019-07-24T22:10:39.551542Z"
    }
   },
   "source": [
    "## Regresión Logísitica\n",
    "\n",
    "Explicación cuática aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x00000232CE393840>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000,\n",
       "                                    multi_class='ovr', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[58  1  6  1  4]\n",
      " [ 5 61  5  1  5]\n",
      " [ 2  0 53  1  2]\n",
      " [ 1  1  2 61  1]\n",
      " [ 2  6  2  1 48]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.85      0.83      0.84        70\n",
      "Internacional       0.88      0.79      0.84        77\n",
      "     Nacional       0.78      0.91      0.84        58\n",
      "      Opinion       0.94      0.92      0.93        66\n",
      "     Sociedad       0.80      0.81      0.81        59\n",
      "\n",
      "     accuracy                           0.85       330\n",
      "    macro avg       0.85      0.85      0.85       330\n",
      " weighted avg       0.85      0.85      0.85       330\n",
      "\n",
      "\n",
      "kappa:0.8142510884174009\n"
     ]
    }
   ],
   "source": [
    "predicted = log_pipe.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "kappa = cohen_kappa_score(y_test, predicted) \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nkappa:'+str(kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.predict([\"En puerto montt se encontró un perrito, que aparentemente, habría consumido drogas de alto calibre. Producto de esto, padecera severa caña durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.predict([\"kim jong un será el próximo candidato a ministro de educación.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de subcategorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Nacional', 'Región Metropolitana', 'Región del Bío Bío',\n",
       "       'Región de los Ríos', 'Región de los Lagos',\n",
       "       'Región de Valparaíso', 'Región de Antofagasta',\n",
       "       'Región del Maule', 'Región de la Araucanía', 'Región de Ñuble',\n",
       "       \"Región de O'Higgins\", 'Región de Aysén', 'Región de Coquimbo',\n",
       "       'Región de Tarapacá', 'Región de Magallanes', 'Región de Atacama',\n",
       "       'Internacional', 'Economía', 'Sociedad', 'Opinión'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(datasets).subcategory.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets_with_subcategories(datasets):\n",
    "    dataset = pd.concat(datasets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.subcategory, test_size=0.33, random_state=42)    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = process_datasets_with_subcategories(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-35-1dd2234e9ec4>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-1dd2234e9ec4>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    log_mod_2 = LogisticRegression(solver='lbfgs', multi_class='ovr', v)\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod_2 = LogisticRegression(solver='lbfgs', multi_class='ovr', v)   \n",
    "log_pipe_2 = Pipeline([('vect', vectorizer), ('clf', log_mod_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = log_pipe_2.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "kappa = cohen_kappa_score(y_test, predicted) \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nkappa:'+str(kappa))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
