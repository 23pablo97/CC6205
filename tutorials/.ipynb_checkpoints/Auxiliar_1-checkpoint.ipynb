{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliar 1 \n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T20:59:04.618627Z",
     "start_time": "2019-07-24T20:59:04.602978Z"
    }
   },
   "source": [
    "## Objetivo del Auxiliar\n",
    "\n",
    "Introducirlos en los primeros temas y herramientas comunes de NLP.\n",
    "\n",
    "Para esto, implementaremos variados modelos de clasificación de texto destinadas a ** predecir la categoría de noticias de la radio biobio**.\n",
    "\n",
    "Las tecnicas que usaremos serán las vistas en clases: \n",
    "\n",
    "- Tokenización, Stemming, Lematización y eliminación de bag of words\n",
    "- Bag of Words\n",
    "- Bayes \n",
    "- Logistic regression \n",
    "\n",
    "Las herramientas que usaremos son (y qué serán **necesarias** para ejecutar este notebook): \n",
    "\n",
    "- Pandas\n",
    "- Scikit-Learn\n",
    "- Spacy\n",
    "- NLTK\n",
    "\n",
    "El auxiliar se encuentra tanto en el [github](https://github.com/dccuchile/CC6205/tree/master/tutorials) del curso, como en un colab de google. \n",
    "\n",
    "Para correr localmente el auxiliar, se recomienda instalar todos los paquetes usando anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:23.406650Z",
     "start_time": "2019-08-08T17:49:23.361514Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bdf04bbe3f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=['ner', 'parser', 'tagger'])\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del texto\n",
    "\n",
    "En esta sección, cargaremos y los procesaremos el dataset de textos con el fin de representar de buena manera los datos para las tareas que debemos realizar. En esta, utilizaremos las siguientes técnicas: \n",
    "\n",
    "- Tokenización\n",
    "- Eliminación de Stopwords\n",
    "- Stemming\n",
    "- Lematización\n",
    "- Transformación de Tokens a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datasets \n",
    "\n",
    "\n",
    "\n",
    "Los datos que usaremos son 5 conjuntos de noticias extraidos de la radio biobio.\n",
    "Cada categoría contiene 200 documentos (noticias). \n",
    "\n",
    "Los cargaremos utilizando la librería pandas: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.423386Z",
     "start_time": "2019-08-08T17:49:23.409130Z"
    }
   },
   "outputs": [],
   "source": [
    "nacional = pd.read_json(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/datasets/biobio_nacional.json\", encoding ='utf-8')\n",
    "internacional = pd.read_json(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/datasets/biobio_internacional.json\", encoding ='utf-8')\n",
    "economia = pd.read_json(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/datasets/biobio_economia.json\", encoding ='utf-8')\n",
    "sociedad = pd.read_json(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/datasets/biobio_sociedad.json\", encoding ='utf-8')\n",
    "opinion = pd.read_json(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/datasets/biobio_opinion.json\", encoding ='utf-8')\n",
    "\n",
    "datasets = [nacional, internacional, economia, sociedad, opinion]\n",
    "dataset = pd.concat(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualizar que es lo que cargamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.486135Z",
     "start_time": "2019-08-08T17:49:24.428342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_hour</th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>42</td>\n",
       "      <td>167</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>26/06/2019</td>\n",
       "      <td>16:45</td>\n",
       "      <td>César Vega Martínez</td>\n",
       "      <td>/lista/autores/cevega</td>\n",
       "      <td>Mujer fue detenida en EEUU: habría obligado a ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/d...</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publication_date publication_hour               author  \\\n",
       "count               200              200                  200   \n",
       "unique               42              167                   13   \n",
       "top          26/06/2019            16:45  César Vega Martínez   \n",
       "freq                 10                3                   78   \n",
       "\n",
       "                  author_link  \\\n",
       "count                     200   \n",
       "unique                     13   \n",
       "top     /lista/autores/cevega   \n",
       "freq                       78   \n",
       "\n",
       "                                                    title  \\\n",
       "count                                                 200   \n",
       "unique                                                200   \n",
       "top     Mujer fue detenida en EEUU: habría obligado a ...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                     link  category  \\\n",
       "count                                                 200       200   \n",
       "unique                                                200         1   \n",
       "top     https://www.biobiochile.cl/noticias/sociedad/d...  Sociedad   \n",
       "freq                                                    1       200   \n",
       "\n",
       "       subcategory content tags embedded_links  \n",
       "count          200     200  200            200  \n",
       "unique           1     196  191              3  \n",
       "top       Sociedad           []             []  \n",
       "freq           200       5    9            198  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Por ejemplo, examinemos las columnas del dataset de noticias de la categoría sociedad \n",
    "# (Todos los datasets tienen el mismo formato)\n",
    "sociedad.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de noticia de categoría sociedad: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.501015Z",
     "start_time": "2019-08-08T17:49:24.489105Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extraigamos una noticia de ejemplo desde el dataset\n",
    "sample = sociedad.iloc[19:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.530769Z",
     "start_time": "2019-08-08T17:49:24.503488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>publication_hour</th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>02/08/2019</td>\n",
       "      <td>15:08</td>\n",
       "      <td>Emilio Contreras</td>\n",
       "      <td>/lista/autores/Econtreras</td>\n",
       "      <td>Chile deja de utilizar 16.170 toneladas de bol...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/sociedad/d...</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Sociedad</td>\n",
       "      <td>Chile dejó de utilizar 16.170 toneladas de b...</td>\n",
       "      <td>[#16.170 toneladas, #balance, #bolsas plástica...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publication_date publication_hour            author  \\\n",
       "19       02/08/2019            15:08  Emilio Contreras   \n",
       "\n",
       "                  author_link  \\\n",
       "19  /lista/autores/Econtreras   \n",
       "\n",
       "                                                title  \\\n",
       "19  Chile deja de utilizar 16.170 toneladas de bol...   \n",
       "\n",
       "                                                 link  category subcategory  \\\n",
       "19  https://www.biobiochile.cl/noticias/sociedad/d...  Sociedad    Sociedad   \n",
       "\n",
       "                                              content  \\\n",
       "19    Chile dejó de utilizar 16.170 toneladas de b...   \n",
       "\n",
       "                                                 tags embedded_links  \n",
       "19  [#16.170 toneladas, #balance, #bolsas plástica...             []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.546144Z",
     "start_time": "2019-08-08T17:49:24.534240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extraemos el contenido y la categoría de la noticia seleccionada.\n",
    "sample_content = sample.content.values[0]\n",
    "sample_category = sample.category.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.561556Z",
     "start_time": "2019-08-08T17:49:24.548155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido:\n",
      "\n",
      " Chile dejó de utilizar 16.170 toneladas de bolsas plásticas desde que hace un año implementó una ley que prohíbe su entrega en supermercados y retails, informó este viernes el ministerio del Medio Ambiente.  Tras un periodo de prueba de seis meses, el gobierno chileno puso en vigencia en agosto del año pasado la ley que evitó el consumo de unas 2.200 millones de bolsas plásticas, una reducción significativa tomando en cuenta que, hasta la promulgación de la norma, Chile producía 3.200 millones de bolsas anuales .  “Si consideramos el peso de estas bolsas que se dejaron de entregar, unas 16.170 toneladas, equivalen a 13.940 autos”, indicó el ministerio en un comunicado.  Desde la puesta en marcha de la norma, los chilenos asumieron el hábito de utilizar bolsas reutilizables, de tela o material reciclable, lo cual ha colaborado en reducir la contaminación que producen los sacos plásticos principalmente en los océanos, en los que yacen unos 13 millones de toneladas de plásticos a nivel mundial.        Presentan proyecto para obligar al comercio a entregar gratis bolsas reciclables    La prohibición “genera un cambio de hábitos muy importante, algo que pensábamos que era imposible hacer en corto tiempo ya se ha hecho posible” , afirmó la ministra de Medio Ambiente, Carolina Schmidt , a medios locales.  Schimdt explicó que para el próximo año la ley se extenderá a pequeños almacenes y locales de barrio, que actualmente sólo pueden entregar dos bolsas por comprador.  Chile también se puso la meta de incrementar en un 60% el reciclaje de envases y embalajes de plásticos que en la actualidad alcanza al 12% .  Para 2025 , en tanto, el gobierno espera que el 100% de los envases de plásticos sean reutilizables o biodegradables, “un cambio radical” , según Schmidt.  Según la ONU, 5 billones de bolsas de plástico se consumen anualmente en el mundo, la mayoría hechas de polietileno, un derivado del petróleo que demora cerca de 500 años en biodegradarse. \n",
      "\n",
      "Clase Correspondiente:\n",
      "\n",
      " Sociedad\n"
     ]
    }
   ],
   "source": [
    "print(\"Contenido:\\n\\n\", sample_content.strip(),\n",
    "      \"\\n\\nClase Correspondiente:\\n\\n\", sample_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar\n",
    "\n",
    "¿Qué era tokenizar?\n",
    "\n",
    "    In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning).\n",
    "    \n",
    "Referencia: [Tokenización en wikipedia](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy y el objeto nlp\n",
    "\n",
    "`nlp` es el objeto que nos permite usar e interactuar con la librería [`spacy`](https://spacy.io/).\n",
    "Esta librería incluye variadas herramientras, tales como tokenizar, lematizar, descartar stopwords, entre otras (para este auxiliar, solo utilizaremos las mencionadas). El objeto nlp lo instanciamos en la sección de imports.\n",
    "\n",
    "Para usarla, simplemente se le pasa el texto como parámetro, como veremos en el siguiente ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.592271Z",
     "start_time": "2019-08-08T17:49:24.564496Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n",
      "gentíl\n",
      "ciudadano\n",
      ",\n",
      "¿\n",
      "qué\n",
      "tal\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for word in nlp(\"hola gentíl ciudadano, ¿qué tal?\"):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizemos la noticia de ejemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.607663Z",
     "start_time": "2019-08-08T17:49:24.595248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_content = [word.text for word in nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación: \n",
    "\n",
    "    tokenized_content = [word.text for word in nlp(sample_content)] \n",
    "\n",
    "Equivale a :\n",
    "\n",
    "    tokenized_content = []\n",
    "    for word in nlp(sample_content):\n",
    "        tokenized_content.append(word.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos como quedó el texto tokenizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.623025Z",
     "start_time": "2019-08-08T17:49:24.610130Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dejó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utilizar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toneladas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plásticas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0           \n",
       "1      Chile\n",
       "2       dejó\n",
       "3         de\n",
       "4   utilizar\n",
       "5     16.170\n",
       "6  toneladas\n",
       "7         de\n",
       "8     bolsas\n",
       "9  plásticas"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokenized_content)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que es básicamente, un arreglo (o vector) con las palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.638401Z",
     "start_time": "2019-08-08T17:49:24.625008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chile',\n",
       " 'dejó',\n",
       " 'de',\n",
       " 'utilizar',\n",
       " '16.170',\n",
       " 'toneladas',\n",
       " 'de',\n",
       " 'bolsas',\n",
       " 'plásticas']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_content[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords \n",
    "\n",
    "¿Qué eran las stopwords?\n",
    "\n",
    "    In computing, stop words are words which are filtered out before or after processing of natural language data (text).[1] Stop words are generally the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools avoid removing stop words to support phrase search. \n",
    "    \n",
    "Referencias: [Stopwords en Wikipedia](https://en.wikipedia.org/wiki/Stop_words)\n",
    "\n",
    "En este caso, utilizaremos las stopwords inlcuidas en la librería spaCy en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.653776Z",
     "start_time": "2019-08-08T17:49:24.640384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.669153Z",
     "start_time": "2019-08-08T17:49:24.655761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>cosas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>supuesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>trabaja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>solos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>buenas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>mío</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>aún</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>poca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>soyos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "95    general\n",
       "69      cosas\n",
       "13   supuesto\n",
       "98    trabaja\n",
       "253     solos\n",
       "268    buenas\n",
       "480       mío\n",
       "306       aún\n",
       "196      poca\n",
       "50      soyos"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(STOP_WORDS).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover las stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.684528Z",
     "start_time": "2019-08-08T17:49:24.673121Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_content_no_stop_words = [\n",
    "    token for token in tokenized_content if token not in STOP_WORDS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.699904Z",
     "start_time": "2019-08-08T17:49:24.687504Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>no stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dejó</td>\n",
       "      <td>utilizar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utilizar</td>\n",
       "      <td>toneladas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.170</td>\n",
       "      <td>bolsas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toneladas</td>\n",
       "      <td>plásticas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>año</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bolsas</td>\n",
       "      <td>implementó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plásticas</td>\n",
       "      <td>ley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    original no stopwords\n",
       "1      Chile        Chile\n",
       "2       dejó     utilizar\n",
       "3         de       16.170\n",
       "4   utilizar    toneladas\n",
       "5     16.170       bolsas\n",
       "6  toneladas    plásticas\n",
       "7         de          año\n",
       "8     bolsas   implementó\n",
       "9  plásticas          ley"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(tokenized_content, tokenized_content_no_stop_words),\n",
    "             columns=['original', 'no stopwords'])[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "¿Qué era el stemming? \n",
    "\n",
    "    Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n",
    "    \n",
    "Referencia: [Stemming en Wikipedia](https://en.wikipedia.org/wiki/Stemming)\n",
    "  \n",
    "#### Ejemplos: \n",
    "\n",
    "\n",
    "| word | stem of the word  |\n",
    "|---|---|\n",
    "working | work\n",
    "worked | work\n",
    "works | work\n",
    "\n",
    "#### nltk\n",
    "\n",
    "En este caso, utilizaremos la segunda librería de herramientas de nlp: [`nltk`](https://www.nltk.org/). Esta provee una buena herramienta para hacer stemming en español : `SnowballStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.730656Z",
     "start_time": "2019-08-08T17:49:24.701888Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "stemmed_content = [stemmer.stem(word) for word in tokenized_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.746032Z",
     "start_time": "2019-08-08T17:49:24.732640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "      <td>chil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dejó</td>\n",
       "      <td>dej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utilizar</td>\n",
       "      <td>utiliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.170</td>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toneladas</td>\n",
       "      <td>tonel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bolsas</td>\n",
       "      <td>bols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plásticas</td>\n",
       "      <td>plastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    original     stem\n",
       "1      Chile     chil\n",
       "2       dejó      dej\n",
       "3         de       de\n",
       "4   utilizar   utiliz\n",
       "5     16.170   16.170\n",
       "6  toneladas    tonel\n",
       "7         de       de\n",
       "8     bolsas     bols\n",
       "9  plásticas  plastic"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(tokenized_content, stemmed_content),\n",
    "             columns=['original', 'stem'])[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "¿Qué era lematización? \n",
    "\n",
    "    \n",
    "    Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.[\n",
    "    \n",
    "    \n",
    "Referencia: [Lematización en wikipedia](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    \n",
    "#### Ejemplos\n",
    "\n",
    "| word | lemma  |\n",
    "|---|---|\n",
    "dije| decir \n",
    "guapas | guapo\n",
    "mesa | mesas\n",
    "\n",
    "\n",
    "#### Lematizar el texto\n",
    "\n",
    "Al igual que la tokenización, utilizaremos `scpaCy` (a través del objeto `nlp`) para lematizar el contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.761408Z",
     "start_time": "2019-08-08T17:49:24.748512Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lemmatized_content = [word.lemma_ for word in nlp(sample_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.776784Z",
     "start_time": "2019-08-08T17:49:24.763888Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dejó</td>\n",
       "      <td>dejar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utilizar</td>\n",
       "      <td>utilizar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.170</td>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toneladas</td>\n",
       "      <td>tonelada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bolsas</td>\n",
       "      <td>bolsa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plásticas</td>\n",
       "      <td>plástico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    original     lemma\n",
       "1      Chile     Chile\n",
       "2       dejó     dejar\n",
       "3         de        de\n",
       "4   utilizar  utilizar\n",
       "5     16.170    16.170\n",
       "6  toneladas  tonelada\n",
       "7         de        de\n",
       "8     bolsas     bolsa\n",
       "9  plásticas  plástico"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizar la lematización\n",
    "pd.DataFrame(zip(tokenized_content, lemmatized_content),\n",
    "             columns=['original', 'lemma'])[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparativa entre stemming y lematizar\n",
    "\n",
    "Discusión: \n",
    "\n",
    "    ¿Cuál es mejor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.792160Z",
     "start_time": "2019-08-08T17:49:24.779264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile</td>\n",
       "      <td>chil</td>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dejó</td>\n",
       "      <td>dej</td>\n",
       "      <td>dejar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utilizar</td>\n",
       "      <td>utiliz</td>\n",
       "      <td>utilizar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.170</td>\n",
       "      <td>16.170</td>\n",
       "      <td>16.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>toneladas</td>\n",
       "      <td>tonel</td>\n",
       "      <td>tonelada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bolsas</td>\n",
       "      <td>bols</td>\n",
       "      <td>bolsa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plásticas</td>\n",
       "      <td>plastic</td>\n",
       "      <td>plástico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>desde</td>\n",
       "      <td>desd</td>\n",
       "      <td>desde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hace</td>\n",
       "      <td>hac</td>\n",
       "      <td>hacer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>un</td>\n",
       "      <td>un</td>\n",
       "      <td>uno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>año</td>\n",
       "      <td>año</td>\n",
       "      <td>año</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     original     stem     lemma\n",
       "1       Chile     chil     Chile\n",
       "2        dejó      dej     dejar\n",
       "3          de       de        de\n",
       "4    utilizar   utiliz  utilizar\n",
       "5      16.170   16.170    16.170\n",
       "6   toneladas    tonel  tonelada\n",
       "7          de       de        de\n",
       "8      bolsas     bols     bolsa\n",
       "9   plásticas  plastic  plástico\n",
       "10      desde     desd     desde\n",
       "11        que      que       que\n",
       "12       hace      hac     hacer\n",
       "13         un       un       uno\n",
       "14        año      año       año"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(tokenized_content, stemmed_content, lemmatized_content),\n",
    "             columns=['original', 'stem', 'lemma'])[1:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "¿Qué es?\n",
    "\n",
    "\n",
    "    \n",
    "    The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words \n",
    "    \n",
    "Referencia: [BoW en wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "- Doc1 : 'I love dogs'\n",
    "- Doc2: 'I hate dogs and knitting.\n",
    "- Doc3: 'Knitting is my hobby and my passion.\n",
    "\n",
    "![BOW](https://i1.wp.com/datameetsmedia.com/wp-content/uploads/2017/05/bagofwords.004.jpeg)\n",
    "\n",
    "### scikit-learn y CountVectorizer\n",
    "\n",
    "Para transformar los tokens al modelo `bag of words (BoW)` usaremos la librería [`scikit-learn`](https://scikit-learn.org/stable/), específicamente, la clase [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Esta requiere que definamos tokenizadores. Es decir, funciones que tomen un documento y lo retornen como una lista de palabras. \n",
    "\n",
    "En los tokenizadores podemos definir si dejar o quitar stopwords, lematizar, hacer stemming, entre otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizadores para CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.807536Z",
     "start_time": "2019-08-08T17:49:24.794144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizers for CountVectorizer\n",
    "\n",
    "\n",
    "# Solo tokenizar el doc.\n",
    "def tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc)]\n",
    "\n",
    "\n",
    "# Tokenizar y remover las stopwords del doc\n",
    "def tokenizer_with_stopwords(doc):\n",
    "    return [x.orth_ for x in nlp(doc) if x.orth_ not in STOP_WORDS]\n",
    "\n",
    "\n",
    "# Tokenizar y lematizar.\n",
    "def tokenizer_with_lemmatization(doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "\n",
    "# Tokenizar y hacer stemming.\n",
    "def tokenizer_with_stemming(doc):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return [stemmer.stem(word) for word in [x.orth_ for x in nlp(doc)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.853664Z",
     "start_time": "2019-08-08T17:49:24.809520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x574 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 153 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanciamos CountVectorizer con el tokenizador seleccionado. \n",
    "# Definimos si queremos n-gramas en el último parámetro.\n",
    "vectorizer = CountVectorizer(analyzer='word',\n",
    "                             tokenizer=tokenizer,\n",
    "                             ngram_range=(1, 1))\n",
    "\n",
    "# Extraemos 4 noticias desde opinion y se la entregamos al vectorizador para que las transforme a vectores BoW.\n",
    "bow = vectorizer.fit_transform(opinion.sample(4).content)\n",
    "\n",
    "# Examinamos el primero\n",
    "bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.869041Z",
     "start_time": "2019-08-08T17:49:24.858624Z"
    }
   },
   "outputs": [],
   "source": [
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.884416Z",
     "start_time": "2019-08-08T17:49:24.873504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  2,  1,  1,  1,  1, 13,  0,  8,  1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizemos el BoW generado para este caso\n",
    "bow[0].toarray()[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de Texto\n",
    "\n",
    "### ¿En qué consiste la clasificación de texto?\n",
    "\n",
    "Según [wikipedia](https://en.wikipedia.org/wiki/Document_classification): \n",
    "\n",
    "    \"The task is to assign a document to one or more classes or categories\"\n",
    "\n",
    "\n",
    "Algunos ejemplos:\n",
    "\n",
    "- Assigning subject categories, **topics**, or genres\n",
    "- Spam detection \n",
    "- Authorship identification\n",
    "- Age/gender identification\n",
    "- Language Identification\n",
    "- Sentiment analysis\n",
    "- ...\n",
    "\n",
    "### Definición formal\n",
    "\n",
    "Input:\n",
    "- A document    $d$\n",
    "- A fixed set of classes $C=\\{c_{1},    c_{2},...,    c_{J}\\}$\n",
    "\n",
    "Output:    \n",
    "\n",
    "- A predicted class $c \\in C$\n",
    "\n",
    "### Tipos de técnicas de clasificación: \n",
    "\n",
    "- Hand-coded Rules. (No se verán en este aux).\n",
    "- Supervised Machine Learning: \n",
    "    - Naïve Bayes\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Muchos mas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar los datasets\n",
    "\n",
    "Seleccionar solo las columnas relevantes y divider en conjuntos de entrenamiento y de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.899792Z",
     "start_time": "2019-08-08T17:49:24.886897Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_datasets(dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.category, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.915168Z",
     "start_time": "2019-08-08T17:49:24.902272Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = process_datasets(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de tópicos de noticias con Naive Bayes\n",
    "\n",
    "\n",
    "¿Qué es?\n",
    "\n",
    "    In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. \n",
    "\n",
    "- Simple (“naïve”) classification method based on Bayes rule that relies on very simple representation of document: *Bag of words*:\n",
    "\n",
    "Given a problem instance to be classified, represented by a vector $x =(x_{1},\\dots ,x_{n})$ representing some n features (independent variables), it assigns to this instance probabilities:\n",
    "\n",
    "$$ p(C_k | x_1, \\dots, x_n) $$\n",
    "\n",
    "or each of $K$ possible outcomes or classes $ C_{k}$.\n",
    "\n",
    "\n",
    "Using Bayes' theorem, the conditional probability can be decomposed as \n",
    "\n",
    "$$ p(C_k | x ) = \\frac{p (C_k) p(x | C_k)}{p(x)}$$\n",
    "\n",
    "In plain english:\n",
    "\n",
    "$$posterior = \\frac{prior * likehood}{evidence} $$\n",
    "\n",
    "\n",
    "#### En este caso...\n",
    "\n",
    "$$ p(\\ nacional\\ |\\  [0,0,3,0,6,0,2,\\dots] ) = \\frac{p (\\ nacional\\ )\\ p([0,0,3,0,6,0,2,\\dots]\\ | \\ nacional)}{p([0,0,3,0,6,0,2,\\dots])}$$\n",
    "\n",
    "\n",
    "Referencia : [Naive Bayes Classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "\n",
    "Observación: Dado el supuesto de independencia del clasificador, no se deben utilizar n-gramas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establecer el Pipeline\n",
    "\n",
    "\n",
    "Un [`pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) es la definición del proceso que llevará a cabo el programa para preprocesarlo y despues clasificarlo. \n",
    "Puede tener múltiples etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:24.930544Z",
     "start_time": "2019-08-08T17:49:24.917152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "# Definimos el vectorizador para convertir el texto a BoW:\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer = TOKENIZER, ngram_range=(1,1))  \n",
    "\n",
    "# Definimos el clasificador que usaremos.\n",
    "clf = MultinomialNB()   \n",
    "\n",
    "# Definimos el pipeline\n",
    "text_clf = Pipeline([('vect', vectorizer), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar\n",
    "\n",
    "Entrenamos el nuevo clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:27.912497Z",
     "start_time": "2019-08-08T17:49:24.933030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x000001F2C8991F28>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar\n",
    "\n",
    "Evaluamos el rendimiento del clasificador que acabamos de entrenar, a traves de:\n",
    "\n",
    "- [`Matriz de confusión`](https://es.wikipedia.org/wiki/Matriz_de_confusi%C3%B3n)\n",
    "- [`Indice F1`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) (Puntaje, mientras mas alto mejor)\n",
    "\n",
    "En la matriz de confusión: \n",
    "\n",
    "- `precision` is the number of correct results divided by the number of all returned results. \n",
    "- `recall` is the number of correct results divided by the number of results that should have been returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:29.158337Z",
     "start_time": "2019-08-08T17:49:27.914482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[62  0  3  5  0]\n",
      " [ 2 68  2  3  2]\n",
      " [ 1  0 41 15  1]\n",
      " [ 4  1  0 61  0]\n",
      " [ 1  6  0  4 48]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.89      0.89      0.89        70\n",
      "Internacional       0.91      0.88      0.89        77\n",
      "     Nacional       0.89      0.71      0.79        58\n",
      "      Opinion       0.69      0.92      0.79        66\n",
      "     Sociedad       0.94      0.81      0.87        59\n",
      "\n",
      "     accuracy                           0.85       330\n",
      "    macro avg       0.86      0.84      0.85       330\n",
      " weighted avg       0.86      0.85      0.85       330\n",
      "\n",
      "\n",
      "F1:0.8467695462432305\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "score = f1_score(y_test, predicted, average='macro') \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1:'+str(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:29.173753Z",
     "start_time": "2019-08-08T17:49:29.160820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype='<U13')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"En puerto montt se encontró un perrito, que aparentemente, habría consumido drogas de alto calibre. Producto de esto, padecera severa caña durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:29.189132Z",
     "start_time": "2019-08-08T17:49:29.176697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Internacional'], dtype='<U13')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"kim jong un será el próximo candidato a ministro de educación.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:29.204469Z",
     "start_time": "2019-08-08T17:49:29.191076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Economia'], dtype='<U13')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"El banco mundial presentó para chile un decrecimiento económico de 92% y una inflación de 8239832983289%.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T22:10:39.567162Z",
     "start_time": "2019-07-24T22:10:39.551542Z"
    }
   },
   "source": [
    "### Clasificación de tópicos de noticias con Regresión Logísitica\n",
    "\n",
    "No profundizaremos en este clasificador, mas del hecho de que se \"supone\" que debería tener mejor rendimiento que el de bayes.\n",
    "\n",
    "Referencia: [Regresión Logística](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establecer el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:29.219843Z",
     "start_time": "2019-08-08T17:49:29.207444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:33.757254Z",
     "start_time": "2019-08-08T17:49:29.222820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x000001F2C8991F28>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000,\n",
       "                                    multi_class='ovr', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:35.048878Z",
     "start_time": "2019-08-08T17:49:33.759237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[60  0  8  2  0]\n",
      " [ 5 63  4  1  4]\n",
      " [ 2  0 51  3  2]\n",
      " [ 1  2  0 62  1]\n",
      " [ 4  5  1  3 46]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Economia       0.83      0.86      0.85        70\n",
      "Internacional       0.90      0.82      0.86        77\n",
      "     Nacional       0.80      0.88      0.84        58\n",
      "      Opinion       0.87      0.94      0.91        66\n",
      "     Sociedad       0.87      0.78      0.82        59\n",
      "\n",
      "     accuracy                           0.85       330\n",
      "    macro avg       0.85      0.85      0.85       330\n",
      " weighted avg       0.86      0.85      0.85       330\n",
      "\n",
      "\n",
      "F1 Score: 0.8529633827856452\n"
     ]
    }
   ],
   "source": [
    "predicted = log_pipe.predict(X_test)\n",
    "\n",
    "conf = confusion_matrix(y_test, predicted)\n",
    "score = f1_score(y_test, predicted, average='macro') \n",
    "class_rep = classification_report(y_test, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1 Score: '+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:35.064253Z",
     "start_time": "2019-08-08T17:49:35.052808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.predict([\"En puerto montt se encontró un perrito, que aparentemente, habría consumido drogas de alto calibre. Producto de esto, padecera severa caña durante varios dias.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:35.079590Z",
     "start_time": "2019-08-08T17:49:35.067689Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sociedad'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.predict([\"kim jong un será el próximo candidato a ministro de educación.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de Autoría de documentos\n",
    "\n",
    "¿Existirá un patrón en como escriben los periodistas que nos permitan identificarlos a partir de sus textos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:35.094993Z",
     "start_time": "2019-08-08T17:49:35.081575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gonzalo Cifuentes', 'Felipe Delgado', 'Nicolás Parra',\n",
       "       'Catalina Díaz', 'Valentina González', 'Emilio Lara',\n",
       "       'Manuel Stuardo', 'Nicolás Díaz', 'Sandar Oporto',\n",
       "       'María José Villarroel', 'Catalina Sánchez', 'Matías Vega',\n",
       "       'Manuel Cabrera', 'Periodismo UCSC', 'Diego Vera', 'Yerko Roa',\n",
       "       'Felipe Díaz Montero', 'Ariela Muñoz', 'Yessenia Márquez',\n",
       "       'Gerson Guzmán D.', 'Paola Alemán', 'Sebastián Asencio',\n",
       "       'Claudia Miño', 'Camilo Suazo', 'Verónica Reyes', 'Max Duhalde',\n",
       "       'Francisca Rivas', 'Hernán Bustamante', 'Leonardo Casas',\n",
       "       'Alberto González', 'Jonathan Flores', 'Scarlet Stuardo',\n",
       "       'Gerson Guzmán', 'Bernardita Villa', 'César Vega Martínez',\n",
       "       'Camila Álvarez', 'Jaime Parra', 'Emilio Contreras',\n",
       "       'Fabián Barría', 'Denisse Charpentier', 'Nicole Briones', 'Tu Voz',\n",
       "       'Natalia Muñoz', 'Tamara Rojas', 'Alejandra Soto', 'Pablo Cabeza'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(datasets).author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:35.110369Z",
     "start_time": "2019-08-08T17:49:35.097447Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_datasets_by_author(datasets):\n",
    "    dataset = pd.concat(datasets)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.content, dataset.author, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:35.125718Z",
     "start_time": "2019-08-08T17:49:35.112823Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = process_datasets_by_author(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:35.141098Z",
     "start_time": "2019-08-08T17:49:35.127703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Qué tokenizer usaremos?\n",
    "TOKENIZER = tokenizer_with_lemmatization\n",
    "\n",
    "log_mod_by_author = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \n",
    "log_pipe_by_author = Pipeline([('vect', vectorizer), ('clf', log_mod_by_author)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:48.030156Z",
     "start_time": "2019-08-08T17:49:35.143575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_with_lemmatization at 0x000001F2C8991F28>,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000,\n",
       "                                    multi_class='ovr', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe_by_author.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T17:49:49.275803Z",
     "start_time": "2019-08-08T17:49:48.032140Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\pablo\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for Logistic Regression + ngram features:\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  2 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 32  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      "Classification Report\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Alejandra Soto       0.00      0.00      0.00         3\n",
      "         Ariela Muñoz       0.00      0.00      0.00         3\n",
      "     Bernardita Villa       0.40      0.50      0.44         4\n",
      "       Camila Álvarez       1.00      1.00      1.00         1\n",
      "         Camilo Suazo       0.33      0.11      0.17         9\n",
      "        Catalina Díaz       0.00      0.00      0.00         3\n",
      "         Claudia Miño       0.00      0.00      0.00         3\n",
      "  César Vega Martínez       0.69      0.80      0.74        25\n",
      "  Denisse Charpentier       0.00      0.00      0.00         1\n",
      "           Diego Vera       0.46      0.73      0.56        44\n",
      "     Emilio Contreras       0.00      0.00      0.00         2\n",
      "          Emilio Lara       0.50      0.17      0.25        12\n",
      "       Felipe Delgado       0.06      0.20      0.09         5\n",
      "  Felipe Díaz Montero       0.25      0.33      0.29         3\n",
      "      Francisca Rivas       0.00      0.00      0.00         7\n",
      "        Gerson Guzmán       0.00      0.00      0.00         1\n",
      "     Gerson Guzmán D.       0.00      0.00      0.00         2\n",
      "    Gonzalo Cifuentes       0.33      0.12      0.18         8\n",
      "    Hernán Bustamante       0.00      0.00      0.00         2\n",
      "      Jonathan Flores       0.00      0.00      0.00         4\n",
      "       Leonardo Casas       0.00      0.00      0.00         0\n",
      "       Manuel Cabrera       0.12      0.10      0.11        10\n",
      "       Manuel Stuardo       0.00      0.00      0.00         2\n",
      "María José Villarroel       0.74      0.89      0.81        36\n",
      "          Matías Vega       0.27      0.23      0.25        13\n",
      "          Max Duhalde       0.00      0.00      0.00         1\n",
      "       Nicole Briones       0.00      0.00      0.00         1\n",
      "         Nicolás Díaz       0.00      0.00      0.00         6\n",
      "        Nicolás Parra       0.20      0.12      0.15         8\n",
      "         Paola Alemán       0.00      0.00      0.00         4\n",
      "        Sandar Oporto       0.33      0.10      0.15        10\n",
      "      Scarlet Stuardo       0.75      0.80      0.77        15\n",
      "    Sebastián Asencio       0.00      0.00      0.00         2\n",
      "         Tamara Rojas       0.00      0.00      0.00         5\n",
      "               Tu Voz       1.00      0.73      0.84        11\n",
      "   Valentina González       0.42      0.67      0.52        21\n",
      "       Verónica Reyes       0.57      0.80      0.67        40\n",
      "            Yerko Roa       0.00      0.00      0.00         1\n",
      "     Yessenia Márquez       0.00      0.00      0.00         2\n",
      "\n",
      "             accuracy                           0.50       330\n",
      "            macro avg       0.22      0.22      0.21       330\n",
      "         weighted avg       0.43      0.50      0.45       330\n",
      "\n",
      "\n",
      "F1 Score:0.20508098327455018\n"
     ]
    }
   ],
   "source": [
    "predicted = log_pipe_by_author.predict(X_test_2)\n",
    "\n",
    "conf = confusion_matrix(y_test_2, predicted)\n",
    "score = f1_score(y_test_2, predicted, average='macro') \n",
    "class_rep = classification_report(y_test_2, predicted)\n",
    "\n",
    "print('\\nConfusion Matrix for Logistic Regression + ngram features:')\n",
    "print(conf)\n",
    "print('\\nClassification Report')\n",
    "print(class_rep)\n",
    "print('\\nF1 Score:'+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:03.045562Z",
     "start_time": "2019-08-08T13:28:03.029690Z"
    }
   },
   "source": [
    "## Créditos\n",
    "\n",
    "Todas las noticias extraidas perteneces a [Biobio Chile](https://www.biobiochile.cl/), los cuales gentilmente licencian todo su material a través de la [licencia Creative Commons (CC-BY-NC)](https://creativecommons.org/licenses/by-nc/2.0/cl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:28:21.929923Z",
     "start_time": "2019-08-08T13:28:21.917993Z"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "Gitgub del curso: \n",
    "- https://github.com/dccuchile/CC6205\n",
    "\n",
    "Slides:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf\n",
    "\n",
    "\n",
    "Análisis de sentimientos como clasificación de texto:\n",
    "- https://affectivetweets.cms.waikato.ac.nz/benchmark/\n",
    "\n",
    "Algunos Recursos útiles\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Scikit-learn Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "- [Spacy Tutorial](https://www.datacamp.com/community/blog/spacy-cheatsheet)\n",
    "- [NLTK Cheat sheet](http://sapir.psych.wisc.edu/programming_for_psychologists/cheat_sheets/Text-Analysis-with-NLTK-Cheatsheet.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "770px",
    "left": "1561px",
    "top": "111.133px",
    "width": "359px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
