{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introducción\" data-toc-modified-id=\"Introducción-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introducción</a></span></li><li><span><a href=\"#Representaciones\" data-toc-modified-id=\"Representaciones-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Representaciones</a></span></li><li><span><a href=\"#Algoritmos\" data-toc-modified-id=\"Algoritmos-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Algoritmos</a></span></li><li><span><a href=\"#Métricas-de-Evaluación\" data-toc-modified-id=\"Métricas-de-Evaluación-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Métricas de Evaluación</a></span></li><li><span><a href=\"#Experimentos\" data-toc-modified-id=\"Experimentos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Experimentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importar-librerías-y-utiles\" data-toc-modified-id=\"Importar-librerías-y-utiles-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Importar librerías y utiles</a></span></li><li><span><a href=\"#Definir-métodos-de-evaluación\" data-toc-modified-id=\"Definir-métodos-de-evaluación-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Definir métodos de evaluación</a></span></li><li><span><a href=\"#Datos\" data-toc-modified-id=\"Datos-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Datos</a></span></li><li><span><a href=\"#Analizar-los-datos\" data-toc-modified-id=\"Analizar-los-datos-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Analizar los datos</a></span></li><li><span><a href=\"#Custom-Features\" data-toc-modified-id=\"Custom-Features-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Custom Features</a></span></li><li><span><a href=\"#Definir-la-representación-y-el-clasificador\" data-toc-modified-id=\"Definir-la-representación-y-el-clasificador-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Definir la representación y el clasificador</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-para-algún-dataset\" data-toc-modified-id=\"Ejecutar-el-pipeline-para-algún-dataset-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Ejecutar el pipeline para algún dataset</a></span></li><li><span><a href=\"#Ejecutar-el-pipeline-por-cada-train-set\" data-toc-modified-id=\"Ejecutar-el-pipeline-por-cada-train-set-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Ejecutar el pipeline por cada train set</a></span></li><li><span><a href=\"#Predecir-target-set\" data-toc-modified-id=\"Predecir-target-set-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Predecir target set</a></span></li></ul></li><li><span><a href=\"#Conclusiones\" data-toc-modified-id=\"Conclusiones-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusiones</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:08.174519Z",
     "start_time": "2020-03-31T13:49:08.165989Z"
    }
   },
   "source": [
    "# Tarea 1 NLP\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Nombre:**\n",
    "\n",
    "- **Usuario o nombre de equipo en Codalab:** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:49:47.464702Z",
     "start_time": "2020-03-31T13:49:47.444379Z"
    }
   },
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    }
   },
   "source": [
    "Este es el baseline de la tarea 1. Contiene la estructura del reporte mas un código básico que:\n",
    "\n",
    "- Obtiene los dataset.\n",
    "- Divide los datasets en train (entrenamiento y prueba) y target set (el que clasificar para subir a la competencia).\n",
    "- Crea un Pipeline que: \n",
    "    - Crea features personalizadas.\n",
    "    - Transforma los dataset a bag of words(BoW).  \n",
    "    - Entrena un clasificador usando cada train set.\n",
    "- Clasifica y evalua el sistema creado usando el test set.\n",
    "- Clasifica el target set.\n",
    "- Genera una submission con el target en formato zip en el directorio en donde se está ejecutando el notebook. \n",
    "\n",
    "La idea es que a partir de este notebook puedan empezar la tarea. Obviamente, no es requisito usarlo, pero puede facilitarles mucho al comienzo.\n",
    "\n",
    "Para ver las instrucciones, descarga el enunciado de la tarea en ucursos. **¡Recuerda ingresar tu nombre, el de tu equipo y sus usuarios en codalab!**\n",
    "\n",
    "(Pueden eliminar esta y cualquier celda con instrucciones...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:34:25.683540Z",
     "start_time": "2020-03-31T13:34:25.673430Z"
    }
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:13.474238Z",
     "start_time": "2020-03-31T13:47:13.454068Z"
    }
   },
   "source": [
    "## Representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:17.719268Z",
     "start_time": "2020-03-31T13:47:17.709207Z"
    }
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:47:52.064631Z",
     "start_time": "2020-03-31T13:47:52.044451Z"
    }
   },
   "source": [
    "## Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC: ...\n",
    "- Kappa: ...\n",
    "- Accuracy: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T13:31:40.023344Z",
     "start_time": "2020-03-31T13:31:40.003541Z"
    }
   },
   "source": [
    "### Importar librerías y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.166420Z",
     "start_time": "2020-04-01T15:32:27.826255Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir métodos de evaluación\n",
    "\n",
    "Estas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.186328Z",
     "start_time": "2020-04-01T15:32:29.166420Z"
    }
   },
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
    "\n",
    "    # Classification Report\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report:\\n')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "    predicted_probabilities = predicted_probabilities[:, [\n",
    "        labels.index('low'),\n",
    "        labels.index('medium'),\n",
    "        labels.index('high')\n",
    "    ]]\n",
    "    \n",
    "    # Auc\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
    "    \n",
    "    # Kappa\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end='\\t')\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    print('------------------------------------------------------\\n')\n",
    "    return np.array([auc, kappa, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos\n",
    "\n",
    "Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.686681Z",
     "start_time": "2020-04-01T15:32:29.186328Z"
    }
   },
   "outputs": [],
   "source": [
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.726401Z",
     "start_time": "2020-04-01T15:32:29.686681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>10704</td>\n",
       "      <td>@MeghanEMurphy Oh gosh, if you get 800+ raging...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>10422</td>\n",
       "      <td>Drop Snapchat names #bored #snap #swap #pics</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>10056</td>\n",
       "      <td>Absolutely fuming that some woman jumped into ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>10291</td>\n",
       "      <td>#Anger or #wrath is an intense emotional respo...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>10474</td>\n",
       "      <td>Puzzle investing opening portland feodal popul...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>10142</td>\n",
       "      <td>i had an hour of football practice under the b...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>10612</td>\n",
       "      <td>Because you offended by the lies of Alexandria...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>10716</td>\n",
       "      <td>Praying for the #Lord to keep #anger #hate #je...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>10303</td>\n",
       "      <td>Yo there's a kid on my snap chat from LA &amp;amp;...</td>\n",
       "      <td>anger</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10047</td>\n",
       "      <td>Still can't log into my fucking Snapchat #Snap...</td>\n",
       "      <td>anger</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet  class  \\\n",
       "704  10704  @MeghanEMurphy Oh gosh, if you get 800+ raging...  anger   \n",
       "422  10422       Drop Snapchat names #bored #snap #swap #pics  anger   \n",
       "56   10056  Absolutely fuming that some woman jumped into ...  anger   \n",
       "291  10291  #Anger or #wrath is an intense emotional respo...  anger   \n",
       "474  10474  Puzzle investing opening portland feodal popul...  anger   \n",
       "142  10142  i had an hour of football practice under the b...  anger   \n",
       "612  10612  Because you offended by the lies of Alexandria...  anger   \n",
       "716  10716  Praying for the #Lord to keep #anger #hate #je...  anger   \n",
       "303  10303  Yo there's a kid on my snap chat from LA &amp;...  anger   \n",
       "47   10047  Still can't log into my fucking Snapchat #Snap...  anger   \n",
       "\n",
       "    sentiment_intensity  \n",
       "704              medium  \n",
       "422              medium  \n",
       "56                 high  \n",
       "291              medium  \n",
       "474              medium  \n",
       "142                high  \n",
       "612              medium  \n",
       "716              medium  \n",
       "303              medium  \n",
       "47                 high  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de algunas filas aleatorias:\n",
    "train['anger'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento. Noten que las clases están desbalanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.766213Z",
     "start_time": "2020-04-01T15:32:29.726401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617 \n",
      "---------------------------------------\n",
      "\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699 \n",
      "---------------------------------------\n",
      "\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488 \n",
      "---------------------------------------\n",
      "\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453 \n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count(),\n",
    "          '\\n---------------------------------------\\n')\n",
    "\n",
    "\n",
    "for dataset_name in train:\n",
    "    get_group_dist(dataset_name, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Features \n",
    "\n",
    "Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n",
    "\n",
    "1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta carácteres relevantes ('!', '?', '#') en los tweets.\n",
    "2. Definios una función cómo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n",
    "3. Hacemos un override de la función `transform` en donde iteramos por cada tweet, llamamos a la función que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n",
    "\n",
    "Esto nos facilitará el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aquí](https://scikit-learn.org/stable/data_transforms.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.786237Z",
     "start_time": "2020-04-01T15:32:29.766213Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count('#')\n",
    "        num_exclamations = tweet.count('!')\n",
    "        num_interrogations = tweet.count('?')\n",
    "        return [num_hashtags, num_exclamations, num_interrogations]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        for tweet in X:\n",
    "            chars.append(self.get_relevant_chars(tweet))\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.806148Z",
     "start_time": "2020-04-01T15:32:29.786237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just joined #pottermore and was sorted into HU...</td>\n",
       "      <td>[2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I get so angry at people that don't know that ...</td>\n",
       "      <td>[2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kingcharles9th i Lowkey forgot you had twitte...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@canada4trumpnow @donlemon he has BAD TEMPERAM...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im so angry 😂🙃</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Is it me, or is Ding wearing the look of a man...</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Petition for non-Maghrebi PoC to stop buying '...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@Idubbbz @LeafyIsHere  I am offended</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ananya just grabbed a bible, opened it, starte...</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@johnaugust @clmazin I was so looking forward ...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          1\n",
       "0  Just joined #pottermore and was sorted into HU...  [2, 0, 0]\n",
       "1  I get so angry at people that don't know that ...  [2, 0, 0]\n",
       "2  @kingcharles9th i Lowkey forgot you had twitte...  [1, 0, 0]\n",
       "3  @canada4trumpnow @donlemon he has BAD TEMPERAM...  [1, 0, 0]\n",
       "4                                     Im so angry 😂🙃  [0, 0, 0]\n",
       "5  Is it me, or is Ding wearing the look of a man...  [1, 0, 1]\n",
       "6  Petition for non-Maghrebi PoC to stop buying '...  [0, 0, 0]\n",
       "7               @Idubbbz @LeafyIsHere  I am offended  [0, 0, 0]\n",
       "8  Ananya just grabbed a bible, opened it, starte...  [0, 0, 1]\n",
       "9  @johnaugust @clmazin I was so looking forward ...  [1, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos que sucede si ejecutamos el transformer\n",
    "sample = train['anger'].sample(10).tweet\n",
    "pd.DataFrame(zip(sample, CharsCountTransformer().transform(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir la representación y el clasificador\n",
    "\n",
    "Para definir el flujo de los datos sobre nuestras transformaciones y el clasificador usaremos un `Pipeline`.\n",
    "\n",
    "Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el proceso que seguiran nuestros datos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n",
    "\n",
    "El pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n",
    "\n",
    "\n",
    "        Dataset -> BagOfWords (llamado CountVectorizer) -> NaiveBayes\n",
    "\n",
    "En código queda cómo:\n",
    "\n",
    "```python\n",
    "    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "```\n",
    "\n",
    "\n",
    "Ahora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar los Transformer en un solo vector.\n",
    "\n",
    "\n",
    "        Dataset -> FeatureUnion [CountVectorizer, CharsCountTransformer] -> NaiveBayes\n",
    "\n",
    "\n",
    "En código queda como: \n",
    "\n",
    "```python\n",
    "Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count',CharsCountTransformer())])),\n",
    "            ('clf', MultinomialNB())])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas pistas sobre como mejorar el rendimiento del vectorizador y del clasificador:\n",
    "\n",
    "- **Vectorizador**: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. También, el parámetro `ngram_range` (Ojo que el clf naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia). Además, implementar los atributos que crean útiles desde el listado del el enunciado. Investigar también el vectorizador tf-idf.\n",
    "\n",
    "- **Clasificador**: investigar otros clasificadores mas efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la función `predict_proba`).\n",
    "\n",
    "- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Pueden guiarse (pero no limitarse) por las que dice el enunciado.\n",
    "\n",
    "- **Reducción de dimensionalidad**: También puede serles de ayuda. Referencias [aquí](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n",
    "\n",
    "- Por último, pueden encontrar mas referencias de cómo mejorar sus features, el vectorizador y el clasificador [aquí](https://affectivetweets.cms.waikato.ac.nz/benchmark/).\n",
    "\n",
    "Recuerden que cada vez que ejecuten pipeline, deben hacerlo con una instancia distinta. De lo contrario, podrían solapar resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el pipeline para algún dataset\n",
    "\n",
    "Ejecuta el pipeline sobre un dataset. Retorna el modelo ya entrenado mas sus labels asociadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.826406Z",
     "start_time": "2020-04-01T15:32:29.806148Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_experiment_0_pipeline():\n",
    "    return Pipeline([('features',\n",
    "                      FeatureUnion([('bow', CountVectorizer()),\n",
    "                                    ('chars_count', CharsCountTransformer())\n",
    "                                    ])), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:29.846382Z",
     "start_time": "2020-04-01T15:32:29.826406Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(dataset, dataset_name):\n",
    "\n",
    "    # Dividimos el dataset en train y test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "\n",
    "    pipeline = get_experiment_0_pipeline()\n",
    "\n",
    "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "    # Obtenemos el orden de las clases aprendidas.\n",
    "    learned_labels = pipeline.classes_\n",
    "\n",
    "    # Evaluamos:\n",
    "    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
    "    return pipeline, learned_labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar el pipeline por cada train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:30.066135Z",
     "start_time": "2020-04-01T15:32:29.846382Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  7  47   0]\n",
      " [  5 193   8]\n",
      " [  0  43   8]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.58      0.13      0.21        54\n",
      "      medium       0.68      0.94      0.79       206\n",
      "        high       0.50      0.16      0.24        51\n",
      "\n",
      "    accuracy                           0.67       311\n",
      "   macro avg       0.59      0.41      0.41       311\n",
      "weighted avg       0.64      0.67      0.60       311\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.615\tKappa: 0.133\tAccuracy: 0.669\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 22  69   2]\n",
      " [  8 211  14]\n",
      " [  2  70  17]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.69      0.24      0.35        93\n",
      "      medium       0.60      0.91      0.72       233\n",
      "        high       0.52      0.19      0.28        89\n",
      "\n",
      "    accuracy                           0.60       415\n",
      "   macro avg       0.60      0.44      0.45       415\n",
      "weighted avg       0.60      0.60      0.55       415\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.672\tKappa: 0.192\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[ 14  68   2]\n",
      " [  9 141  13]\n",
      " [  0  24  27]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.61      0.17      0.26        84\n",
      "      medium       0.61      0.87      0.71       163\n",
      "        high       0.64      0.53      0.58        51\n",
      "\n",
      "    accuracy                           0.61       298\n",
      "   macro avg       0.62      0.52      0.52       298\n",
      "weighted avg       0.61      0.61      0.56       298\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.729\tKappa: 0.261\tAccuracy: 0.611\n",
      "------------------------------------------------------\n",
      "\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[ 14  52   1]\n",
      " [ 14 144   7]\n",
      " [  0  39  13]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.50      0.21      0.29        67\n",
      "      medium       0.61      0.87      0.72       165\n",
      "        high       0.62      0.25      0.36        52\n",
      "\n",
      "    accuracy                           0.60       284\n",
      "   macro avg       0.58      0.44      0.46       284\n",
      "weighted avg       0.59      0.60      0.55       284\n",
      "\n",
      "Scores:\n",
      "\n",
      "AUC:  0.633\tKappa: 0.175\tAccuracy: 0.602\n",
      "------------------------------------------------------\n",
      "\n",
      "Average scores:\n",
      "\n",
      " Average AUC: 0.662\t Average Kappa: 0.19\t Average Accuracy: 0.621\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "scores_array = []\n",
    "\n",
    "# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
    "for dataset_name, dataset in train.items():\n",
    "    # ejecutamos el pipeline sobre el dataset\n",
    "    classifier, learned_labels, scores = run(dataset, dataset_name)\n",
    "\n",
    "    # guardamos el clasificador entrenado\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "    # guardamos las labels aprendidas por el clasificador\n",
    "    learned_labels_array.append(learned_labels)\n",
    "\n",
    "    # guardamos los scores obtenidos\n",
    "    scores_array.append(scores)\n",
    "\n",
    "# print avg scores\n",
    "print(\n",
    "    \"Average scores:\\n\\n\",\n",
    "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
    "    .format(*np.array(scores_array).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    }
   },
   "source": [
    "### Predecir target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:30.086102Z",
     "start_time": "2020-04-01T15:32:30.066135Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar las columnas\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T15:32:30.296588Z",
     "start_time": "2020-04-01T15:32:30.086102Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "# Crear carpeta ./predictions\n",
    "if (not os.path.isdir('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "# por cada target set:\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecirlo\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones en archivo separado\n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
